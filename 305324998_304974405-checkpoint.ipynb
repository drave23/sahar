{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYL2yhps-MoR"
   },
   "source": [
    "<h1> ~Neural Collaborative Filtering -  Implementation with Keras </h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 2668.0,
     "status": "ok",
     "timestamp": 1.53280798018E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "Qar2rpog-MoS",
    "outputId": "1b22139b-f649-45f1-cb03-4d16657f3bf7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqaKXWj2-MoX"
   },
   "source": [
    "**Define dataset folder and files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ATWd9BvF-MoX"
   },
   "outputs": [],
   "source": [
    "RATING_DATA_FILE_TRAIN = 'u1.base'\n",
    "RATING_DATA_FILE_TEST = 'u1.test'\n",
    "MOVIES_DATA_FILE_PATH = 'u.item'\n",
    "USERS_DATA_FILE_PATH = 'u.user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "jYlcT_ptB7Wu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 24)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets explore movies.csv\n",
    "m_cols = ['movie_id','movie title','release date','video release date','IMDb URL'\n",
    "          ,'unknown','Action','Adventure','Animation','Childrens','Comedy',\n",
    "          'Crime','Documentary','Drama','Fantasy','Film-Noir','Horror','Musical','Mystery','Romance','Sci-Fi',\n",
    "        'Thriller','War','Western']\n",
    "data= pd.read_csv('d:/documents/users/ravedan/Downloads/ml-100k/u.item',delimiter='|',names=m_cols,encoding='latin-1')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore users\n",
    "u_cols = ['user_id','age','gender','occupation','zip_code']\n",
    "data_u = pd.read_csv('d:/documents/users/ravedan/Downloads/ml-100k/u.user',delimiter='|',names=u_cols,encoding='latin-1')\n",
    "data_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_cols = ['user_id','movie_id','rating','timestamp']\n",
    "data_r = pd.read_csv('d:/documents/users/ravedan/Downloads/ml-100k/u1.base',delimiter='\\t',names=r_cols,encoding='latin-1')\n",
    "data_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFNtJREFUeJzt3X+0XWV95/H3R0DsqrYViTHDj0ZbqkXHQSZlqYDNiAIC\nFRiQH7PagmUW1qFS18xqjU6nddpFm850rNXWaVNFqC2/LKIICCpigVqFgKgQQBmJS1hgAirSqlTg\nO3+cHT3GJ/eeG7LPPsl9v9a6657z7L3P/t6Hy/3k2T+enapCkqTNPWnoAiRJs8mAkCQ1GRCSpCYD\nQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKlp56ELeCJ23333Wr58+dBlSNJ25aabbnqgqpbMt952\nHRDLly9n7dq1Q5chSduVJF+ZZD0PMUmSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLU\nZEBIkpq26zuppVm2fNXlC1p//eoje6pE2jqOICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJ\nUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVJTbwGRZK8k1yRZl+S2JL/Zte+W\n5GNJvtR9f/rYNm9OcleSO5Mc1ldtkqT59TmCeBT4b1W1L/Bi4Iwk+wKrgKurah/g6u493bKTgOcD\nhwPvSrJTj/VJkubQW0BU1X1VdXP3+mHgdmAP4Gjg3G61c4FjutdHAxdU1SNVdTdwF3BAX/VJkuY2\nlXMQSZYDLwI+Ayytqvu6RfcDS7vXewBfHdvsnq5t8886PcnaJGs3btzYW82StNj1HhBJngpcDLyx\nqr41vqyqCqiFfF5VramqFVW1YsmSJduwUknSuF4DIskujMLh76rqA13z15Is65YvAzZ07fcCe41t\nvmfXJkkaQJ9XMQV4D3B7Vb1tbNGlwCnd61OAD421n5Rk1yTPBvYBbuirPknS3Hbu8bMPBH4F+EKS\nW7q2twCrgYuSnAZ8BTgBoKpuS3IRsI7RFVBnVNVjPdYnSZpDbwFRVdcD2cLiQ7awzVnAWX3VJEma\nnHdSS5KaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElN\nBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRA\nSJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQk\nqWnnoQuQtPWWr7p8wdusX31kD5VoR9TbCCLJ2Uk2JLl1rO2tSe5Nckv3dcTYsjcnuSvJnUkO66su\nSdJk+jzEdA5weKP9T6tqv+7rCoAk+wInAc/vtnlXkp16rE2SNI/eAqKqrgW+PuHqRwMXVNUjVXU3\ncBdwQF+1SZLmN8RJ6jck+Xx3COrpXdsewFfH1rmna/sRSU5PsjbJ2o0bN/ZdqyQtWtMOiP8LPAfY\nD7gP+D8L/YCqWlNVK6pqxZIlS7Z1fZKkzlQDoqq+VlWPVdXjwF/zg8NI9wJ7ja26Z9cmSRrIVAMi\nybKxt8cCm65wuhQ4KcmuSZ4N7APcMM3aJEk/bN77IJL8DHBPVT2SZCXwQuBvquqb82x3PrAS2D3J\nPcDvASuT7AcUsB54HUBV3ZbkImAd8ChwRlU9trU/lCTpiZvkRrmLgRVJfhZYA3wIOA84Yq6Nqurk\nRvN75lj/LOCsCeqRJE3BJAHxeFU9muRY4J1V9c4kn+27MEn9WOjd1955vXhNcg7ie0lOBk4BLuva\ndumvJEnSLJgkIF4LvAQ4q6ru7k4iv6/fsiRJQ5v3EFNVrUvyJmDv7v3dwB/3XZgkaVjzjiCS/BJw\nC3Bl936/JJf2XZgkaViTnKR+K6Mb2j4JUFW3JHlOjzVJi9LWTN0t9WmSgPheVT2UZLzt8Z7qkTRj\nfObE4jVJQNyW5D8BOyXZBzgT+FS/ZUmShjbJVUxvYPSchkeA84FvAW/ssyhJ0vAmuYrp28B/774k\nSYvEFgMiydur6o1JPsxo7qQfUlWv7rUySdKg5hpBbLoZ7k+mUYgkabZsMSCq6qbu5TOAy6vqkemU\nJEmaBZOcpP4l4ItJ3pfkqCSTXPkkSdrOzRsQVfVa4GeB9wMnA/8vybv7LkySNKyJRgNV9b0kH2F0\nsvrHgGOA/9xnYZKkYU0yF9OrkpwDfAk4Dng38Kye65IkDWySEcSvAhcCr/NEtSQtHpPcKHdykqXA\nK7v5mG6oqg29VyZJGtQkh5heA9wAvAY4AfhMkuP7LkySNKxJDjH9DvALm0YNSZYAHwf+vs/CJEnD\nmuQ+iCdtdkjpwQm3kyRtxyYZQVyZ5CpGM7kCnAhc0V9JkqRZMMlJ6t9KchxwYNe0pqou6bcsSdLQ\nJr1R7mLg4p5rkSTNkLmm+36YxjTfQICqqp/orSpJ0uDmGkFczeiO6Q8AF1bVV6ZTkiRpFmzxaqSq\nOgY4DNgIrEnyD0n+S5LdpladJGkwc16uWlUPVdV7gVcBfwX8PnDqFOqSJA1szpPUSV7KaIrvg4Hr\ngWOr6rppFCZJGtZcJ6nXA98ELgBOBx7t2vcHqKqbp1CfJGkgc40g1jO6iukw4FBGVy9tUsDL+ytL\n6s/yVZcveJv1q4/soRJpts31TOqVU6xDkjRjfL60pJmw0JGdo7r+OemeJKlpiwGR5MDu+67TK0eS\nNCvmGkG8o/v+T9MoRJI0W+Y6B/G9JGuAPZK8Y/OFVXXmXB+c5GzgKGBDVb2ga9uN0fOtlzO6SuqE\nqvpGt+zNwGnAY8CZVXXVgn8aSdI2M9cI4ijgE8B3gZsaX/M5Bzh8s7ZVwNVVtQ+juZ5WASTZFzgJ\neH63zbuS7DTxTyFJ2ubmusz1AeCCJLdX1ecW+sFVdW2S5Zs1Hw2s7F6fC3wSeFPXfkFVPQLcneQu\n4AA8vCVJg5nkKqYHk1ySZEP3dXGSPbdyf0ur6r7u9f3A0u71HsBXx9a7p2uTJA1kkoB4L3Ap8G+6\nrw93bU9IVRXt503MKcnpSdYmWbtx48YnWoYkaQsmCYhnVtV7q+rR7uscYMlW7u9rSZYBdN83dO33\nAnuNrbdn1/YjqmpNVa2oqhVLlmxtGZKk+UxyJ/UDSX4ZOL97fzLw4Fbu71LgFGB19/1DY+3nJXkb\no1HKPsANW7kPSQPbmvmuNHsmGUH8GnACo3MG9wHHA6+db6Mk5zM6yfzcJPckOY1RMLwyyZeAV3Tv\nqarbgIuAdcCVwBlV9djCfxxJ0rYy7wiie9Toqxf6wVV18hYWHbKF9c8CzlrofiRJ/XAuJklSkwEh\nSWoyICRJTfMGRJLfGXvtzK6StEjMNd33m5K8hNFVS5s49YUkLRJzXcV0B/Aa4DlJruvePyPJc6vq\nzqlUJ0kazFwB8U3gLYwm11sJ/DxwKLCqC4mX9l6dNAFvypL6MVdAHAb8LvAzwNuAzwP/UlXz3iQn\nSdr+bfEcRFW9paoOYfRgn/cBOwFLklyf5MNTqk+SNJBJ5mK6qqrWAmuTvL6qDkqye9+FSZKGNclU\nG7899vbUru2BvgqSZpHnObQYTTKC+L6tebKcdhwL/SO5fvWRPVUiaRq8k1qS1GRASJKaDAhJUpMB\nIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCS\npCYDQpLUtKAnykl989Ge0uxwBCFJajIgJElNBoQkqcmAkCQ1LeqT1As9Ibp+9ZE9VSJJs8cRhCSp\nyYCQJDUZEJKkpkHOQSRZDzwMPAY8WlUrkuwGXAgsB9YDJ1TVN4aoT5I07AjiP1TVflW1onu/Cri6\nqvYBru7eS5IGMkuHmI4Gzu1enwscM2AtkrToDRUQBXw8yU1JTu/allbVfd3r+4Glw5QmSYLh7oM4\nqKruTfJM4GNJ7hhfWFWVpFobdoFyOsDee+/df6WStEgNEhBVdW/3fUOSS4ADgK8lWVZV9yVZBmzY\nwrZrgDUAK1asaIaIZoMzs0rbt6kfYkry40metuk1cChwK3ApcEq32inAh6ZdmyTpB4YYQSwFLkmy\naf/nVdWVSW4ELkpyGvAV4IQBapMkdaYeEFX1ZeDfNdofBA6Zdj2SpLZZusxVkjRDDAhJUpMBIUlq\nMiAkSU0GhCSpyYCQJDUZEJKkJgNCktQ01GR9kvSEbM1cX+tXH9lDJTsuRxCSpCYDQpLUZEBIkpoM\nCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQ\nJDUZEJKkJp8ot0htzdO4JC0ujiAkSU0GhCSpyUNMOwAPF0nqgyMISVKTIwhJi8bWjLbXrz6yh0q2\nDwbEDPKQkaRZ4CEmSVKTIwhJGtisHvpyBCFJajIgJElNHmLqmSecpe3brB7+mYaZG0EkOTzJnUnu\nSrJq6HokabGaqRFEkp2AvwBeCdwD3Jjk0qpaN2xlI44GJC0mMxUQwAHAXVX1ZYAkFwBHAzMREJI0\niR3lH5OzdohpD+CrY+/v6dokSVM2ayOIeSU5HTi9e/vPSe58Ah+3O/DAE69qm7OuhbGuhbGuhZnJ\nuvLHT6iun55kpVkLiHuBvcbe79m1fV9VrQHWbIudJVlbVSu2xWdtS9a1MNa1MNa1MIu5rlk7xHQj\nsE+SZyd5MnAScOnANUnSojRTI4iqejTJbwBXATsBZ1fVbQOXJUmL0kwFBEBVXQFcMaXdbZNDVT2w\nroWxroWxroVZtHWlqvrehyRpOzRr5yAkSTNihw+IJGcn2ZDk1i0sT5J3dFN7fD7J/jNS18okDyW5\npfv63SnUtFeSa5KsS3Jbkt9srDP1/pqwriH66ylJbkjyua6u/9lYZ6jfr0lqm3qfdfvdKclnk1zW\nWDZIf01Q1yB91e17fZIvdPtd21jeX59V1Q79BbwM2B+4dQvLjwA+AgR4MfCZGalrJXDZlPtqGbB/\n9/ppwBeBfYfurwnrGqK/Ajy1e70L8BngxUP31wJqm3qfdfv9r8B5rX0P1V8T1DVIX3X7Xg/sPsfy\n3vpshx9BVNW1wNfnWOVo4G9q5NPATyVZNgN1TV1V3VdVN3evHwZu50fvZJ96f01Y19R1ffDP3dtd\nuq/NT+oN9fs1SW1Tl2RP4Ejg3VtYZZD+mqCuWdZbn+3wATGBWZ7e46XdkPEjSZ4/zR0nWQ68iNG/\nPMcN2l9z1AUD9Fd3WOIWYAPwsaqamf6aoDaYfp+9Hfht4PEtLB+qv+arC4b7/7GAjye5KaOZJDbX\nW58ZELPrZmDvqnoh8E7gg9PacZKnAhcDb6yqb01rv/OZp65B+quqHquq/Rjd9X9AkhdMY7+TmKC2\nqfZZkqOADVV1U5/7WagJ6xrs/0fgoO6/46uAM5K8bFo7NiAmmN5jCFX1rU2HCGp0b8guSXbve79J\ndmH0R/jvquoDjVUG6a/56hqqv8b2/03gGuDwzRYN/vu1pdoG6LMDgVcnWQ9cALw8yd9uts4Q/TVv\nXUP+flXVvd33DcAljGa9HtdbnxkQo6k8frW7EuDFwENVdd/QRSV5VpJ0rw9g9N/qwZ73GeA9wO1V\n9bYtrDb1/pqkroH6a0mSn+pe/xij55jcsdlqg/x+TVLbtPusqt5cVXtW1XJG0+h8oqp+ebPVpt5f\nk9Q1xO9Xt68fT/K0Ta+BQ4HNr3zsrc9m7k7qbS3J+YyuQNg9yT3A7zE6YUdV/SWju7aPAO4Cvg28\ndkbqOh54fZJHge8AJ1V3yUKPDgR+BfhCd+wa4C3A3mN1DdFfk9Q1RH8tA87N6EFXTwIuqqrLkvz6\nWF2D/H5NWNsQffYjZqS/5qtrqL5aClzSZdPOwHlVdeW0+sw7qSVJTR5ikiQ1GRCSpCYDQpLUZEBI\nkpoMCElSkwGhHUKSY5JUkucNXct88sMzg96R5E8m2Ga/JEeMvX91klX9VqrFzoDQjuJk4Pru+xOW\npO97hK7rpk94EXBUkgPnWX8/Rte6A1BVl1bV6j4LlAwIbfe6OZoOAk5jdCfspvYLkhw59v6cJMd3\nk9j97yQ3dpOvva5bvjLJdUkuBdZ1bR/sJkm7bXyitCSnJfliRs9c+Oskf961L0lycffZN873h7+q\nvgPcQje5WpIDkvxTRs8l+FSS5yZ5MvD7wIndqOPEJKeO7fOcjJ4H8KkkX05yfNf+pCTv6kYpH0ty\nxaZl0iR2+DuptSgcDVxZVV9M8mCSf99NvHYhcAJwefdH9hDg9YyC5KGq+oUkuwL/mOSj3WftD7yg\nqu7u3v9aVX29m67ixiQXA7sC/6Nb92HgE8DnuvX/DPjTqro+yd7AVcDPb6nwJE8H9gGu7ZruAA6u\nqkeTvAL4w6o6LqMH1Kyoqt/otjt1s49axigkn8do6oW/B/4jsBzYF3gmo2nSz56kQyUwILRjOJnR\nH2YYTbZ2MnATo4eo/FkXAocD11bVd5IcCrxw7F/TP8noj/S/AjeMhQPAmUmO7V7v1a33LOAfqurr\nAEneD/xct84rgH27qREAfiLJU8eezbDJwUk+133e26vq/rFazk2yD6NpnneZsA8+WFWPA+uSLO3a\nDgLe37Xfn+SaCT9LAgwIbeeS7Aa8HPi3SQrYCagkv1VV303ySeAw4ERG4QGjJ2+9oaqu2uyzVgL/\nstn7VwAvqapvd5/1lHlKehKjJ7d9d571rquqo5I8G/h0kouq6hbgD4BrqurYjJ598cl5PmeTR8Z/\nlAm3kebkOQht744H3ldVP11Vy6tqL+Bu4OBu+YWMJi87GLiya7uK0cRruwAk+blupszN/STwjS4c\nnsfocY4ANwK/mOTp3cns48a2+Sjwhk1vkuw3V/HdaGU18KaxfW6aqvnUsVUfZvS41YX4R+C47lzE\nUkaTQ0oTMyC0vTuZ0Rz54y7mB1czfRT4ReDjVfWvXdu7GZ2EvjnJrcBf0R5NXwnsnOR2Rn/EPw3f\nn5//D4EbGP0RXg881G1zJrCiO/m9Dvj1CX6GvwRe1o0Y/hfwR0k+u1lN1zA6dHVLkhMn+EwY9cM9\n3c/6t4weevPQnFtIY5zNVdoKm84rdCOIS4Czq2rzoBrcWJ3PYBRoB46d75Dm5DkIaeu8tbvK6CmM\nRinTfATlQlyW0YODngz8geGghXAEIUlq8hyEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUtP/Bwk3\ngx5TyNRkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ab87172278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                                 movie title  rating\n",
      "0      1500                   Santa with Muscles (1996)     5.0\n",
      "1      1201  Marlene Dietrich: Shadow and Light (1996)      5.0\n",
      "2      1293                             Star Kid (1997)     5.0\n"
     ]
    }
   ],
   "source": [
    "# 1.1\n",
    "dataRating=data.merge(data_r,on = 'movie_id',how = 'inner')\n",
    "averageMovieRating = dataRating.groupby(['movie_id', 'movie title'])['rating'].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "print('Histogram')\n",
    "plt.hist(avg_movie_rating['rating'],bins='auto')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('# of Movies')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "bestRating = averageMovieRating.head(3)\n",
    "print(bestRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1.2\n",
    "\n",
    "merged_data=dataRating.merge(data_u,on = 'user_id', how='inner')\n",
    "\n",
    "femaleData = merged_data.loc[merged_data['gender'] == 'F']\n",
    "maleData = merged_data.loc[merged_data['gender'] == 'M']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ab87609710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                            movie title gender  ratingFemale\n",
      "0      1472  Visitors, The (Visiteurs, Les) (1993)      F           5.0\n",
      "1      1594                         Everest (1998)      F           5.0\n",
      "2      1599          Someone Else's America (1995)      F           5.0\n"
     ]
    }
   ],
   "source": [
    "#female\n",
    "averageMovieRatingFemale = femaleData.groupby(['movie_id','movie title','gender'])['rating'].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "averageMovieRatingFemale.columns = ['movie_id','movie title','gender', 'ratingFemale']\n",
    "print('Histogram')\n",
    "plt.hist(averageMovieRatingFemale['ratingFemale'],bins='auto')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('# of Movies')\n",
    "plt.show()\n",
    "bestRating = averageMovieRatingFemale.head(3)\n",
    "print(bestRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWv1/mQt3jXXwWBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ab875ed6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                     movie title gender  ratingMale\n",
      "0      1293                 Star Kid (1997)      M         5.0\n",
      "1      1656              Little City (1998)      M         5.0\n",
      "2      1122  They Made Me a Criminal (1939)      M         5.0\n"
     ]
    }
   ],
   "source": [
    "#male\n",
    "\n",
    "averageMovieRatingMale = maleData.groupby(['movie_id','movie title','gender'])['rating'].mean().sort_values(ascending=False).reset_index()\n",
    "averageMovieRatingMale.columns = ['movie_id','movie title','gender', 'ratingMale']\n",
    "print('Histogram')\n",
    "plt.hist(averageMovieRatingMale['ratingMale'],bins='auto')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('# of Movies')\n",
    "plt.show()\n",
    "bestRating = averageMovieRatingMale.head(3)\n",
    "print(bestRating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.2 לענות על הסיף בהתאם לתוצאות שיצאו לנו"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       movie title_x  ratingMale  ratingFemale\n",
      "0                                 Little City (1998)    5.000000      2.000000\n",
      "1                                   Hugo Pool (1997)    5.000000      1.000000\n",
      "2                               Love Serenade (1996)    5.000000      2.333333\n",
      "3                                 Prefontaine (1997)    5.000000      5.000000\n",
      "4                    Letter From Death Row, A (1998)    5.000000      4.000000\n",
      "5                            Leading Man, The (1996)    5.000000      2.500000\n",
      "6                             Quiet Room, The (1996)    5.000000      3.000000\n",
      "7        Two or Three Things I Know About Her (1966)    4.666667      1.000000\n",
      "8                              Close Shave, A (1995)    4.531646      4.714286\n",
      "9                         Wrong Trousers, The (1993)    4.520548      4.541667\n",
      "10                             Grosse Fatigue (1994)    4.500000      2.000000\n",
      "11            Fille seule, La (A Single Girl) (1995)    4.500000      3.000000\n",
      "12                             A Chef in Love (1996)    4.500000      3.750000\n",
      "13                       Little Princess, The (1939)    4.500000      3.000000\n",
      "14               Bitter Sugar (Azucar Amargo) (1996)    4.500000      4.000000\n",
      "15                              Sliding Doors (1998)    4.500000      1.000000\n",
      "16                             Paths of Glory (1957)    4.461538      1.000000\n",
      "17                                 Casablanca (1942)    4.448052      4.414634\n",
      "18                             Third Man, The (1949)    4.440000      3.818182\n",
      "19                           Schindler's List (1993)    4.434286      4.645161\n",
      "20                        Usual Suspects, The (1995)    4.426036      4.285714\n",
      "21                         When We Were Kings (1996)    4.400000      3.555556\n",
      "22                  Shawshank Redemption, The (1994)    4.397727      4.489362\n",
      "23                                Rear Window (1954)    4.385827      4.282609\n",
      "24                                  Star Wars (1977)    4.376045      4.312000\n",
      "25            One Flew Over the Cuckoo's Nest (1975)    4.363057      4.132075\n",
      "26                         North by Northwest (1959)    4.361111      4.064516\n",
      "27                                      Fresh (1994)    4.333333      3.000000\n",
      "28                                  Stonewall (1995)    4.333333      4.000000\n",
      "29                             Sum of Us, The (1994)    4.333333      3.000000\n",
      "...                                              ...         ...           ...\n",
      "1381                    Amityville Curse, The (1990)    1.500000      1.000000\n",
      "1382                             Castle Freak (1995)    1.500000      1.000000\n",
      "1383                                 Hideaway (1995)    1.500000      2.666667\n",
      "1384                                Bad Girls (1994)    1.500000      3.000000\n",
      "1385            Amityville II: The Possession (1982)    1.500000      2.666667\n",
      "1386                          Neon Bible, The (1995)    1.333333      3.000000\n",
      "1387                                 Grease 2 (1982)    1.272727      2.777778\n",
      "1388                                       Ed (1996)    1.250000      1.500000\n",
      "1389                           Amityville 3-D (1983)    1.250000      1.000000\n",
      "1390                              Bushwhacked (1995)    1.250000      1.000000\n",
      "1391      Children of the Corn: The Gathering (1996)    1.100000      1.666667\n",
      "1392              Journey of August King, The (1995)    1.000000      3.000000\n",
      "1393                    Getting Even with Dad (1994)    1.000000      2.000000\n",
      "1394                              Rough Magic (1995)    1.000000      4.000000\n",
      "1395                                Babyfever (1994)    1.000000      1.000000\n",
      "1396                           Simple Wish, A (1997)    1.000000      3.000000\n",
      "1397                             Country Life (1994)    1.000000      2.000000\n",
      "1398             Turbo: A Power Rangers Movie (1997)    1.000000      3.000000\n",
      "1399                     Lay of the Land, The (1997)    1.000000      4.000000\n",
      "1400         Amityville 1992: It's About Time (1992)    1.000000      1.000000\n",
      "1401                    Amityville: Dollhouse (1996)    1.000000      1.000000\n",
      "1402                            Guilty as Sin (1993)    1.000000      2.500000\n",
      "1403             Amityville: A New Generation (1993)    1.000000      1.000000\n",
      "1404                              Mighty, The (1998)    1.000000      1.000000\n",
      "1405     3 Ninjas: High Noon At Mega Mountain (1998)    1.000000      1.000000\n",
      "1406                                    Gordy (1995)    1.000000      1.000000\n",
      "1407                          For Ever Mozart (1996)    1.000000      2.500000\n",
      "1408  Second Jungle Book: Mowgli & Baloo, The (1997)    1.000000      2.500000\n",
      "1409                           Boys in Venice (1996)    1.000000      1.000000\n",
      "1410                  Designated Mourner, The (1997)    1.000000      2.500000\n",
      "\n",
      "[1411 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "males_and_females_ratings = averageMovieRatingMale.merge(averageMovieRatingFemale,on=\"movie_id\")\n",
    "#print(males_and_females_ratings)\n",
    "print(males_and_females_ratings[['movie title_x','ratingMale','ratingFemale']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZsHirou-Mob"
   },
   "source": [
    "**The user and item id for embedding should start from 0. Update ids and save to file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WLCe3LSG-Mob"
   },
   "outputs": [],
   "source": [
    "r_cols = ['userid', 'movieid', 'rating', 'timestamp']\n",
    "ratings_train_df = pd.read_csv(RATING_DATA_FILE_TRAIN, sep='\\t', engine='python', encoding='latin-1',names=r_cols)\n",
    "ratings_train_df['user_emb_id'] = ratings_train_df['userid'] - 1\n",
    "ratings_train_df['movie_emb_id'] = ratings_train_df['movieid'] - 1\n",
    "\n",
    "ratings_test_df = pd.read_csv(RATING_DATA_FILE_TEST, sep='\\t', engine='python', encoding='latin-1',names=r_cols)\n",
    "ratings_test_df['user_emb_id'] = ratings_test_df['userid'] - 1\n",
    "ratings_test_df['movie_emb_id'] = ratings_test_df['movieid'] - 1\n",
    "\n",
    "u_cols = ['userid','age','gender','profession', '']\n",
    "users = pd.read_csv(USERS_DATA_FILE_PATH, sep='|', engine='python', encoding='latin-1',names=u_cols)\n",
    "users['gender'] = users['gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "i_cols = ['movieid', 'title']\n",
    "items_df = pd.read_csv(MOVIES_DATA_FILE_PATH, sep='|', engine='python', encoding='latin-1', names=i_cols, usecols=[0, 1])\n",
    "\n",
    "train_df = pd.merge(items_df, ratings_train_df, on='movieid')\n",
    "test_df = pd.merge(items_df, ratings_test_df, on='movieid')\n",
    "joined_df = pd.concat([train_df, test_df])\n",
    "\n",
    "Users = joined_df['user_emb_id'].values\n",
    "Movies = joined_df['movie_emb_id'].values\n",
    "Ratings = joined_df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 665.0,
     "status": "ok",
     "timestamp": 1.532808041643E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "WFqnpH0gt3B2",
    "outputId": "9ae2e091-9dc6-4b45-d3c3-fe32b69b40dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max user id: 943    Max movie id: 1682\n"
     ]
    }
   ],
   "source": [
    "max_userid = joined_df['userid'].drop_duplicates().max()\n",
    "max_movieid = joined_df['movieid'].drop_duplicates().max()\n",
    "print(\"Max user id:\", max_userid, \"   Max movie id:\", max_movieid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXS_qwtCHTYV"
   },
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9c21Uuhz-Mou"
   },
   "source": [
    "**Define matrix factorization model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "OZbAaKcq-Mou"
   },
   "outputs": [],
   "source": [
    "def get_model(num_users, num_items, latent_dim):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    prediction = merge([user_latent, item_latent], mode = 'dot')\n",
    "    \n",
    "    \n",
    "    model = Model(input=[user_input, item_input], output=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8SECj2K-Mo3"
   },
   "source": [
    "**Define embedding size and compile model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 122.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 738.0,
     "status": "ok",
     "timestamp": 1.532808049353E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "pHMeRWAa1oRm",
    "outputId": "46cc2588-82be-436d-b4b3-fb26202f0923"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"me...)`\n"
     ]
    }
   ],
   "source": [
    "K_LATENT = 20\n",
    "MF_model = get_model(max_userid,max_movieid,K_LATENT)\n",
    "MF_model.compile(loss='mse', optimizer='adamax',metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0i_p3hpsh5BL"
   },
   "source": [
    "**Train model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2638.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 312151.0,
     "status": "ok",
     "timestamp": 1.532808366182E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "gNhPAmChHz18",
    "outputId": "7d9b38ad-4e49-44f4-9365-6966224931e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/80\n",
      "80000/80000 [==============================] - 5s 57us/step - loss: 13.6511 - mean_absolute_error: 3.5216 - val_loss: 13.6325 - val_mean_absolute_error: 3.5086\n",
      "Epoch 2/80\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 12.3171 - mean_absolute_error: 3.3285 - val_loss: 11.4834 - val_mean_absolute_error: 3.1933\n",
      "Epoch 3/80\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 8.4811 - mean_absolute_error: 2.6555 - val_loss: 7.6802 - val_mean_absolute_error: 2.4921\n",
      "Epoch 4/80\n",
      "37952/80000 [=============>................] - ETA: 1s - loss: 5.6958 - mean_absolute_error: 2.051480000/80000 [==============================] - 4s 52us/step - loss: 5.1173 - mean_absolute_error: 1.9107 - val_loss: 5.0138 - val_mean_absolute_error: 1.8835\n",
      "Epoch 5/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 3.4848 - mean_absolute_error: 1.4997 - val_loss: 3.6386 - val_mean_absolute_error: 1.5364\n",
      "Epoch 6/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 2.6117 - mean_absolute_error: 1.2612 - val_loss: 2.8492 - val_mean_absolute_error: 1.3235\n",
      "Epoch 7/80\n",
      "77024/80000 [===========================>..] - ETA: 0s - loss: 2.0973 - mean_absolute_error: 1.113680000/80000 [==============================] - 4s 51us/step - loss: 2.0910 - mean_absolute_error: 1.1119 - val_loss: 2.3512 - val_mean_absolute_error: 1.1841\n",
      "Epoch 8/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 1.7586 - mean_absolute_error: 1.0133 - val_loss: 2.0139 - val_mean_absolute_error: 1.0864\n",
      "Epoch 9/80\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 1.5346 - mean_absolute_error: 0.9443 - val_loss: 1.7772 - val_mean_absolute_error: 1.0153\n",
      "Epoch 10/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 1.3795 - mean_absolute_error: 0.8950 - val_loss: 1.6069 - val_mean_absolute_error: 0.9631\n",
      "Epoch 11/80\n",
      " 2240/80000 [..............................] - ETA: 3s - loss: 1.2985 - mean_absolute_error: 0.864580000/80000 [==============================] - 4s 52us/step - loss: 1.2688 - mean_absolute_error: 0.8592 - val_loss: 1.4791 - val_mean_absolute_error: 0.9236\n",
      "Epoch 12/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 1.1876 - mean_absolute_error: 0.8325 - val_loss: 1.3830 - val_mean_absolute_error: 0.8937\n",
      "Epoch 13/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 1.1269 - mean_absolute_error: 0.8125 - val_loss: 1.3075 - val_mean_absolute_error: 0.8709\n",
      "Epoch 14/80\n",
      "72288/80000 [==========================>...] - ETA: 0s - loss: 1.0845 - mean_absolute_error: 0.798280000/80000 [==============================] - 4s 51us/step - loss: 1.0796 - mean_absolute_error: 0.7966 - val_loss: 1.2483 - val_mean_absolute_error: 0.8526\n",
      "Epoch 15/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 1.0425 - mean_absolute_error: 0.7847 - val_loss: 1.2001 - val_mean_absolute_error: 0.8371\n",
      "Epoch 16/80\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 1.0127 - mean_absolute_error: 0.7747 - val_loss: 1.1606 - val_mean_absolute_error: 0.8249\n",
      "Epoch 17/80\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.9883 - mean_absolute_error: 0.7665 - val_loss: 1.1290 - val_mean_absolute_error: 0.8150\n",
      "Epoch 18/80\n",
      " 2304/80000 [..............................] - ETA: 3s - loss: 0.9369 - mean_absolute_error: 0.735680000/80000 [==============================] - 4s 51us/step - loss: 0.9680 - mean_absolute_error: 0.7597 - val_loss: 1.1025 - val_mean_absolute_error: 0.8065\n",
      "Epoch 19/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.9506 - mean_absolute_error: 0.7537 - val_loss: 1.0803 - val_mean_absolute_error: 0.7992\n",
      "Epoch 20/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.9360 - mean_absolute_error: 0.7488 - val_loss: 1.0610 - val_mean_absolute_error: 0.7931\n",
      "Epoch 21/80\n",
      "72256/80000 [==========================>...] - ETA: 0s - loss: 0.9247 - mean_absolute_error: 0.744580000/80000 [==============================] - 4s 51us/step - loss: 0.9227 - mean_absolute_error: 0.7440 - val_loss: 1.0435 - val_mean_absolute_error: 0.7878\n",
      "Epoch 22/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.9110 - mean_absolute_error: 0.7401 - val_loss: 1.0284 - val_mean_absolute_error: 0.7820\n",
      "Epoch 23/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.9009 - mean_absolute_error: 0.7360 - val_loss: 1.0168 - val_mean_absolute_error: 0.7790\n",
      "Epoch 24/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8914 - mean_absolute_error: 0.7329 - val_loss: 1.0053 - val_mean_absolute_error: 0.7750\n",
      "Epoch 25/80\n",
      " 2208/80000 [..............................] - ETA: 3s - loss: 0.8704 - mean_absolute_error: 0.732480000/80000 [==============================] - 4s 51us/step - loss: 0.8831 - mean_absolute_error: 0.7294 - val_loss: 0.9966 - val_mean_absolute_error: 0.7717\n",
      "Epoch 26/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8753 - mean_absolute_error: 0.7265 - val_loss: 0.9873 - val_mean_absolute_error: 0.7686\n",
      "Epoch 27/80\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.8678 - mean_absolute_error: 0.7236 - val_loss: 0.9802 - val_mean_absolute_error: 0.7666\n",
      "Epoch 28/80\n",
      "70976/80000 [=========================>....] - ETA: 0s - loss: 0.8607 - mean_absolute_error: 0.720680000/80000 [==============================] - 4s 51us/step - loss: 0.8611 - mean_absolute_error: 0.7213 - val_loss: 0.9738 - val_mean_absolute_error: 0.7642\n",
      "Epoch 29/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8548 - mean_absolute_error: 0.7187 - val_loss: 0.9672 - val_mean_absolute_error: 0.7619\n",
      "Epoch 30/80\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.8484 - mean_absolute_error: 0.7162 - val_loss: 0.9622 - val_mean_absolute_error: 0.7600\n",
      "Epoch 31/80\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.8425 - mean_absolute_error: 0.7139 - val_loss: 0.9574 - val_mean_absolute_error: 0.7579\n",
      "Epoch 32/80\n",
      " 2176/80000 [..............................] - ETA: 3s - loss: 0.8070 - mean_absolute_error: 0.705680000/80000 [==============================] - 4s 50us/step - loss: 0.8372 - mean_absolute_error: 0.7114 - val_loss: 0.9519 - val_mean_absolute_error: 0.7563\n",
      "Epoch 33/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8317 - mean_absolute_error: 0.7093 - val_loss: 0.9479 - val_mean_absolute_error: 0.7547\n",
      "Epoch 34/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8263 - mean_absolute_error: 0.7070 - val_loss: 0.9436 - val_mean_absolute_error: 0.7532\n",
      "Epoch 35/80\n",
      "67776/80000 [========================>.....] - ETA: 0s - loss: 0.8212 - mean_absolute_error: 0.705580000/80000 [==============================] - 4s 54us/step - loss: 0.8217 - mean_absolute_error: 0.7049 - val_loss: 0.9402 - val_mean_absolute_error: 0.7523\n",
      "Epoch 36/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8165 - mean_absolute_error: 0.7027 - val_loss: 0.9364 - val_mean_absolute_error: 0.7505\n",
      "Epoch 37/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8119 - mean_absolute_error: 0.7005 - val_loss: 0.9333 - val_mean_absolute_error: 0.7493\n",
      "Epoch 38/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.8068 - mean_absolute_error: 0.6983 - val_loss: 0.9299 - val_mean_absolute_error: 0.7479\n",
      "Epoch 39/80\n",
      "   32/80000 [..............................] - ETA: 14s - loss: 0.6069 - mean_absolute_error: 0.638380000/80000 [==============================] - 4s 51us/step - loss: 0.8022 - mean_absolute_error: 0.6961 - val_loss: 0.9278 - val_mean_absolute_error: 0.7477\n",
      "Epoch 40/80\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.7975 - mean_absolute_error: 0.6942 - val_loss: 0.9247 - val_mean_absolute_error: 0.7463\n",
      "Epoch 41/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7931 - mean_absolute_error: 0.6921 - val_loss: 0.9220 - val_mean_absolute_error: 0.7456\n",
      "Epoch 42/80\n",
      "70304/80000 [=========================>....] - ETA: 0s - loss: 0.7855 - mean_absolute_error: 0.688580000/80000 [==============================] - 4s 51us/step - loss: 0.7887 - mean_absolute_error: 0.6902 - val_loss: 0.9188 - val_mean_absolute_error: 0.7443\n",
      "Epoch 43/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7843 - mean_absolute_error: 0.6881 - val_loss: 0.9167 - val_mean_absolute_error: 0.7433\n",
      "Epoch 44/80\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.7801 - mean_absolute_error: 0.6863 - val_loss: 0.9145 - val_mean_absolute_error: 0.7422\n",
      "Epoch 45/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7759 - mean_absolute_error: 0.6842 - val_loss: 0.9123 - val_mean_absolute_error: 0.7415\n",
      "Epoch 46/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7716 - mean_absolute_error: 0.6820 - val_loss: 0.9110 - val_mean_absolute_error: 0.7414\n",
      "Epoch 47/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7674 - mean_absolute_error: 0.6798 - val_loss: 0.9092 - val_mean_absolute_error: 0.7410\n",
      "Epoch 48/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7632 - mean_absolute_error: 0.6779 - val_loss: 0.9069 - val_mean_absolute_error: 0.7402\n",
      "Epoch 49/80\n",
      "64192/80000 [=======================>......] - ETA: 0s - loss: 0.7593 - mean_absolute_error: 0.676380000/80000 [==============================] - 4s 52us/step - loss: 0.7589 - mean_absolute_error: 0.6761 - val_loss: 0.9046 - val_mean_absolute_error: 0.7389\n",
      "Epoch 50/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7548 - mean_absolute_error: 0.6741 - val_loss: 0.9034 - val_mean_absolute_error: 0.7382\n",
      "Epoch 51/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7508 - mean_absolute_error: 0.6718 - val_loss: 0.9022 - val_mean_absolute_error: 0.7382\n",
      "Epoch 52/80\n",
      "79456/80000 [============================>.] - ETA: 0s - loss: 0.7466 - mean_absolute_error: 0.669880000/80000 [==============================] - 4s 51us/step - loss: 0.7466 - mean_absolute_error: 0.6699 - val_loss: 0.9008 - val_mean_absolute_error: 0.7373\n",
      "Epoch 53/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7424 - mean_absolute_error: 0.6677 - val_loss: 0.8996 - val_mean_absolute_error: 0.7370\n",
      "Epoch 54/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7381 - mean_absolute_error: 0.6657 - val_loss: 0.8973 - val_mean_absolute_error: 0.7362\n",
      "Epoch 55/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7340 - mean_absolute_error: 0.6637 - val_loss: 0.8976 - val_mean_absolute_error: 0.7363\n",
      "Epoch 56/80\n",
      " 1024/80000 [..............................] - ETA: 4s - loss: 0.7573 - mean_absolute_error: 0.6772 80000/80000 [==============================] - 4s 51us/step - loss: 0.7300 - mean_absolute_error: 0.6617 - val_loss: 0.8950 - val_mean_absolute_error: 0.7354\n",
      "Epoch 57/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7256 - mean_absolute_error: 0.6593 - val_loss: 0.8941 - val_mean_absolute_error: 0.7346\n",
      "Epoch 58/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7216 - mean_absolute_error: 0.6571 - val_loss: 0.8931 - val_mean_absolute_error: 0.7348\n",
      "Epoch 59/80\n",
      "69696/80000 [=========================>....] - ETA: 0s - loss: 0.7160 - mean_absolute_error: 0.654380000/80000 [==============================] - 4s 51us/step - loss: 0.7174 - mean_absolute_error: 0.6552 - val_loss: 0.8914 - val_mean_absolute_error: 0.7337\n",
      "Epoch 60/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7131 - mean_absolute_error: 0.6529 - val_loss: 0.8910 - val_mean_absolute_error: 0.7336\n",
      "Epoch 61/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7088 - mean_absolute_error: 0.6508 - val_loss: 0.8892 - val_mean_absolute_error: 0.7329\n",
      "Epoch 62/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.7047 - mean_absolute_error: 0.6485 - val_loss: 0.8884 - val_mean_absolute_error: 0.7327\n",
      "Epoch 63/80\n",
      " 1152/80000 [..............................] - ETA: 4s - loss: 0.7008 - mean_absolute_error: 0.6556 80000/80000 [==============================] - 4s 51us/step - loss: 0.7003 - mean_absolute_error: 0.6464 - val_loss: 0.8879 - val_mean_absolute_error: 0.7322\n",
      "Epoch 64/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6958 - mean_absolute_error: 0.6441 - val_loss: 0.8878 - val_mean_absolute_error: 0.7323\n",
      "Epoch 65/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6916 - mean_absolute_error: 0.6417 - val_loss: 0.8867 - val_mean_absolute_error: 0.7318\n",
      "Epoch 66/80\n",
      "72288/80000 [==========================>...] - ETA: 0s - loss: 0.6871 - mean_absolute_error: 0.639480000/80000 [==============================] - 4s 51us/step - loss: 0.6873 - mean_absolute_error: 0.6393 - val_loss: 0.8857 - val_mean_absolute_error: 0.7314\n",
      "Epoch 67/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6829 - mean_absolute_error: 0.6371 - val_loss: 0.8856 - val_mean_absolute_error: 0.7311\n",
      "Epoch 68/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6785 - mean_absolute_error: 0.6347 - val_loss: 0.8848 - val_mean_absolute_error: 0.7310\n",
      "Epoch 69/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6742 - mean_absolute_error: 0.6323 - val_loss: 0.8841 - val_mean_absolute_error: 0.7306\n",
      "Epoch 70/80\n",
      " 1056/80000 [..............................] - ETA: 4s - loss: 0.7061 - mean_absolute_error: 0.6382 80000/80000 [==============================] - 4s 51us/step - loss: 0.6699 - mean_absolute_error: 0.6302 - val_loss: 0.8834 - val_mean_absolute_error: 0.7307\n",
      "Epoch 71/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6653 - mean_absolute_error: 0.6275 - val_loss: 0.8831 - val_mean_absolute_error: 0.7305\n",
      "Epoch 72/80\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.6609 - mean_absolute_error: 0.6253 - val_loss: 0.8830 - val_mean_absolute_error: 0.7306\n",
      "Epoch 73/80\n",
      "71456/80000 [=========================>....] - ETA: 0s - loss: 0.6556 - mean_absolute_error: 0.622780000/80000 [==============================] - 4s 51us/step - loss: 0.6567 - mean_absolute_error: 0.6228 - val_loss: 0.8821 - val_mean_absolute_error: 0.7303\n",
      "Epoch 74/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6521 - mean_absolute_error: 0.6204 - val_loss: 0.8824 - val_mean_absolute_error: 0.7303\n",
      "Epoch 75/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6477 - mean_absolute_error: 0.6178 - val_loss: 0.8823 - val_mean_absolute_error: 0.7302\n",
      "Epoch 76/80\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.6431 - mean_absolute_error: 0.6154 - val_loss: 0.8832 - val_mean_absolute_error: 0.7303\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping('val_loss', patience=3)]\n",
    "history = MF_model.fit([Users, Movies], Ratings, epochs=80, validation_split=.2, verbose=1, callbacks=callbacks, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_7gdSRqKwL5"
   },
   "source": [
    "**Ex 1 Section A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7bVln6gpIoPT"
   },
   "source": [
    " **Average MAE for standard matrix factorization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 619.0,
     "status": "ok",
     "timestamp": 1.532808382973E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "syBEw8qMIsM4",
    "outputId": "1b7b4adb-dfd7-4be7-f8a2-5448fd566c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.8414171710930373 and Min MAE 0.6153571675896644 in 76 epochs\n"
     ]
    }
   ],
   "source": [
    "print(\"Average MAE:\", np.mean(history.history['mean_absolute_error']),\\\n",
    "      \"and Min MAE\", min(history.history['mean_absolute_error']),\\\n",
    "      \"in\", len(history.history['mean_absolute_error']), \"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_-89O6fINvA"
   },
   "source": [
    "**Define generalized matrix factorization model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "pvUpM0o3-Mox"
   },
   "outputs": [],
   "source": [
    "def get_gmf_model(num_users, num_items, latent_dim,do):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    hidden1 = Multiply()([user_latent, item_latent])\n",
    "    drop = Dropout(do)(hidden1)\n",
    "    prediction = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'prediction')(drop)\n",
    "    \n",
    "    \n",
    "    model = Model(input=[user_input, item_input], output=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGGjA9rRJIXD"
   },
   "source": [
    "** Ex 1 section B **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 4610.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 612118.0,
     "status": "ok",
     "timestamp": 1.532809004288E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "LeEUzV79JGm3",
    "outputId": "08338a76-3920-4bb2-8c5a-b1511cdd3101"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"pr...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 4.7061 - mean_absolute_error: 1.8068 - val_loss: 1.3554 - val_mean_absolute_error: 0.9865\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.2517 - mean_absolute_error: 0.9405 - val_loss: 1.3305 - val_mean_absolute_error: 0.9697\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.2441 - mean_absolute_error: 0.9362 - val_loss: 1.3215 - val_mean_absolute_error: 0.9642\n",
      "Epoch 4/150\n",
      "28736/80000 [=========>....................] - ETA: 2s - loss: 1.2236 - mean_absolute_error: 0.926480000/80000 [==============================] - 4s 54us/step - loss: 1.2158 - mean_absolute_error: 0.9232 - val_loss: 1.2891 - val_mean_absolute_error: 0.9519\n",
      "Epoch 5/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.1540 - mean_absolute_error: 0.8924 - val_loss: 1.2284 - val_mean_absolute_error: 0.9209\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.0930 - mean_absolute_error: 0.8578 - val_loss: 1.1754 - val_mean_absolute_error: 0.8931\n",
      "Epoch 7/150\n",
      "62656/80000 [======================>.......] - ETA: 0s - loss: 1.0578 - mean_absolute_error: 0.836880000/80000 [==============================] - 4s 55us/step - loss: 1.0521 - mean_absolute_error: 0.8338 - val_loss: 1.1387 - val_mean_absolute_error: 0.8717\n",
      "Epoch 8/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.0200 - mean_absolute_error: 0.8152 - val_loss: 1.1093 - val_mean_absolute_error: 0.8538\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9920 - mean_absolute_error: 0.7993 - val_loss: 1.0850 - val_mean_absolute_error: 0.8390\n",
      "Epoch 10/150\n",
      "64832/80000 [=======================>......] - ETA: 0s - loss: 0.9666 - mean_absolute_error: 0.785880000/80000 [==============================] - 4s 56us/step - loss: 0.9681 - mean_absolute_error: 0.7862 - val_loss: 1.0647 - val_mean_absolute_error: 0.8277\n",
      "Epoch 11/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9468 - mean_absolute_error: 0.7752 - val_loss: 1.0470 - val_mean_absolute_error: 0.8184\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9262 - mean_absolute_error: 0.7651 - val_loss: 1.0310 - val_mean_absolute_error: 0.8105\n",
      "Epoch 13/150\n",
      "64576/80000 [=======================>......] - ETA: 0s - loss: 0.9101 - mean_absolute_error: 0.756880000/80000 [==============================] - 4s 55us/step - loss: 0.9094 - mean_absolute_error: 0.7563 - val_loss: 1.0170 - val_mean_absolute_error: 0.8029\n",
      "Epoch 14/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.8949 - mean_absolute_error: 0.7492 - val_loss: 1.0050 - val_mean_absolute_error: 0.7973\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.8754 - mean_absolute_error: 0.7402 - val_loss: 0.9951 - val_mean_absolute_error: 0.7924\n",
      "Epoch 16/150\n",
      "65216/80000 [=======================>......] - ETA: 0s - loss: 0.8655 - mean_absolute_error: 0.734580000/80000 [==============================] - 5s 56us/step - loss: 0.8657 - mean_absolute_error: 0.7353 - val_loss: 0.9863 - val_mean_absolute_error: 0.7881\n",
      "Epoch 17/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.8552 - mean_absolute_error: 0.7308 - val_loss: 0.9786 - val_mean_absolute_error: 0.7850\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.8468 - mean_absolute_error: 0.7270 - val_loss: 0.9717 - val_mean_absolute_error: 0.7808\n",
      "Epoch 19/150\n",
      "64704/80000 [=======================>......] - ETA: 0s - loss: 0.8335 - mean_absolute_error: 0.721280000/80000 [==============================] - 4s 56us/step - loss: 0.8367 - mean_absolute_error: 0.7219 - val_loss: 0.9663 - val_mean_absolute_error: 0.7793\n",
      "Epoch 20/150\n",
      "80000/80000 [==============================] - 5s 57us/step - loss: 0.8296 - mean_absolute_error: 0.7194 - val_loss: 0.9613 - val_mean_absolute_error: 0.7775\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.8258 - mean_absolute_error: 0.7164 - val_loss: 0.9569 - val_mean_absolute_error: 0.7748\n",
      "Epoch 22/150\n",
      "64256/80000 [=======================>......] - ETA: 0s - loss: 0.8157 - mean_absolute_error: 0.711880000/80000 [==============================] - 4s 56us/step - loss: 0.8147 - mean_absolute_error: 0.7113 - val_loss: 0.9529 - val_mean_absolute_error: 0.7729\n",
      "Epoch 23/150\n",
      "80000/80000 [==============================] - 5s 57us/step - loss: 0.8119 - mean_absolute_error: 0.7099 - val_loss: 0.9502 - val_mean_absolute_error: 0.7715\n",
      "Epoch 24/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.8044 - mean_absolute_error: 0.7060 - val_loss: 0.9471 - val_mean_absolute_error: 0.7692\n",
      "Epoch 25/150\n",
      "61536/80000 [======================>.......] - ETA: 0s - loss: 0.7951 - mean_absolute_error: 0.701380000/80000 [==============================] - 5s 57us/step - loss: 0.7967 - mean_absolute_error: 0.7024 - val_loss: 0.9439 - val_mean_absolute_error: 0.7676\n",
      "Epoch 26/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.7952 - mean_absolute_error: 0.7013 - val_loss: 0.9417 - val_mean_absolute_error: 0.7680\n",
      "Epoch 27/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.7905 - mean_absolute_error: 0.6977 - val_loss: 0.9399 - val_mean_absolute_error: 0.7667\n",
      "Epoch 28/150\n",
      "64128/80000 [=======================>......] - ETA: 0s - loss: 0.7844 - mean_absolute_error: 0.696280000/80000 [==============================] - 4s 56us/step - loss: 0.7852 - mean_absolute_error: 0.6968 - val_loss: 0.9379 - val_mean_absolute_error: 0.7655\n",
      "Epoch 29/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.7785 - mean_absolute_error: 0.6934 - val_loss: 0.9360 - val_mean_absolute_error: 0.7642\n",
      "Epoch 30/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7769 - mean_absolute_error: 0.6918 - val_loss: 0.9349 - val_mean_absolute_error: 0.7641\n",
      "Epoch 31/150\n",
      "65952/80000 [=======================>......] - ETA: 0s - loss: 0.7737 - mean_absolute_error: 0.690280000/80000 [==============================] - 4s 55us/step - loss: 0.7731 - mean_absolute_error: 0.6906 - val_loss: 0.9330 - val_mean_absolute_error: 0.7628\n",
      "Epoch 32/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.7713 - mean_absolute_error: 0.6891 - val_loss: 0.9323 - val_mean_absolute_error: 0.7633\n",
      "Epoch 33/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7664 - mean_absolute_error: 0.6864 - val_loss: 0.9309 - val_mean_absolute_error: 0.7623\n",
      "Epoch 34/150\n",
      "66624/80000 [=======================>......] - ETA: 0s - loss: 0.7604 - mean_absolute_error: 0.684680000/80000 [==============================] - 4s 55us/step - loss: 0.7637 - mean_absolute_error: 0.6860 - val_loss: 0.9300 - val_mean_absolute_error: 0.7610\n",
      "Epoch 35/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7607 - mean_absolute_error: 0.6839 - val_loss: 0.9289 - val_mean_absolute_error: 0.7620\n",
      "Epoch 36/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7571 - mean_absolute_error: 0.6826 - val_loss: 0.9277 - val_mean_absolute_error: 0.7604\n",
      "Epoch 37/150\n",
      "67584/80000 [========================>.....] - ETA: 0s - loss: 0.7574 - mean_absolute_error: 0.681880000/80000 [==============================] - 4s 55us/step - loss: 0.7602 - mean_absolute_error: 0.6831 - val_loss: 0.9282 - val_mean_absolute_error: 0.7604\n",
      "Epoch 38/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7560 - mean_absolute_error: 0.6820 - val_loss: 0.9269 - val_mean_absolute_error: 0.7600\n",
      "Epoch 39/150\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.7496 - mean_absolute_error: 0.6780 - val_loss: 0.9262 - val_mean_absolute_error: 0.7601\n",
      "Epoch 40/150\n",
      "67136/80000 [========================>.....] - ETA: 0s - loss: 0.7483 - mean_absolute_error: 0.677880000/80000 [==============================] - 4s 55us/step - loss: 0.7481 - mean_absolute_error: 0.6777 - val_loss: 0.9257 - val_mean_absolute_error: 0.7597\n",
      "Epoch 41/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7441 - mean_absolute_error: 0.6749 - val_loss: 0.9250 - val_mean_absolute_error: 0.7589\n",
      "Epoch 42/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7422 - mean_absolute_error: 0.6741 - val_loss: 0.9246 - val_mean_absolute_error: 0.7593\n",
      "Epoch 43/150\n",
      "68128/80000 [========================>.....] - ETA: 0s - loss: 0.7372 - mean_absolute_error: 0.671780000/80000 [==============================] - 4s 55us/step - loss: 0.7385 - mean_absolute_error: 0.6723 - val_loss: 0.9244 - val_mean_absolute_error: 0.7596\n",
      "Epoch 44/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7406 - mean_absolute_error: 0.6735 - val_loss: 0.9240 - val_mean_absolute_error: 0.7588\n",
      "Epoch 45/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7377 - mean_absolute_error: 0.6710 - val_loss: 0.9236 - val_mean_absolute_error: 0.7578\n",
      "Epoch 46/150\n",
      "68032/80000 [========================>.....] - ETA: 0s - loss: 0.7355 - mean_absolute_error: 0.671380000/80000 [==============================] - 4s 55us/step - loss: 0.7354 - mean_absolute_error: 0.6713 - val_loss: 0.9235 - val_mean_absolute_error: 0.7586\n",
      "Epoch 47/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7349 - mean_absolute_error: 0.6715 - val_loss: 0.9231 - val_mean_absolute_error: 0.7586\n",
      "Epoch 48/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7305 - mean_absolute_error: 0.6691 - val_loss: 0.9221 - val_mean_absolute_error: 0.7577\n",
      "Epoch 49/150\n",
      "68224/80000 [========================>.....] - ETA: 0s - loss: 0.7260 - mean_absolute_error: 0.666980000/80000 [==============================] - 4s 55us/step - loss: 0.7290 - mean_absolute_error: 0.6685 - val_loss: 0.9209 - val_mean_absolute_error: 0.7579\n",
      "Epoch 50/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7256 - mean_absolute_error: 0.6671 - val_loss: 0.9215 - val_mean_absolute_error: 0.7568\n",
      "Epoch 51/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7267 - mean_absolute_error: 0.6666 - val_loss: 0.9206 - val_mean_absolute_error: 0.7569\n",
      "Epoch 52/150\n",
      "67712/80000 [========================>.....] - ETA: 0s - loss: 0.7289 - mean_absolute_error: 0.668080000/80000 [==============================] - 4s 56us/step - loss: 0.7313 - mean_absolute_error: 0.6689 - val_loss: 0.9205 - val_mean_absolute_error: 0.7559\n",
      "Epoch 53/150\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.7269 - mean_absolute_error: 0.6658 - val_loss: 0.9198 - val_mean_absolute_error: 0.7570\n",
      "Epoch 54/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7240 - mean_absolute_error: 0.6643 - val_loss: 0.9200 - val_mean_absolute_error: 0.7565\n",
      "Epoch 55/150\n",
      "68480/80000 [========================>.....] - ETA: 0s - loss: 0.7199 - mean_absolute_error: 0.662680000/80000 [==============================] - 4s 55us/step - loss: 0.7226 - mean_absolute_error: 0.6643 - val_loss: 0.9201 - val_mean_absolute_error: 0.7570\n",
      "Epoch 56/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7243 - mean_absolute_error: 0.6659 - val_loss: 0.9199 - val_mean_absolute_error: 0.7567\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 4.7044 - mean_absolute_error: 1.8032 - val_loss: 1.3475 - val_mean_absolute_error: 0.9830\n",
      "Epoch 2/150\n",
      "47456/80000 [================>.............] - ETA: 1s - loss: 1.2498 - mean_absolute_error: 0.940180000/80000 [==============================] - 5s 59us/step - loss: 1.2507 - mean_absolute_error: 0.9397 - val_loss: 1.3290 - val_mean_absolute_error: 0.9679\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 1.2386 - mean_absolute_error: 0.9338 - val_loss: 1.3134 - val_mean_absolute_error: 0.9607\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 1.1955 - mean_absolute_error: 0.9136 - val_loss: 1.2647 - val_mean_absolute_error: 0.9403\n",
      "Epoch 5/150\n",
      "50432/80000 [=================>............] - ETA: 1s - loss: 1.1377 - mean_absolute_error: 0.883880000/80000 [==============================] - 5s 60us/step - loss: 1.1201 - mean_absolute_error: 0.8746 - val_loss: 1.1969 - val_mean_absolute_error: 0.9047\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 1.0568 - mean_absolute_error: 0.8383 - val_loss: 1.1447 - val_mean_absolute_error: 0.8749\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 1.0087 - mean_absolute_error: 0.8106 - val_loss: 1.1051 - val_mean_absolute_error: 0.8522\n",
      "Epoch 8/150\n",
      "51552/80000 [==================>...........] - ETA: 1s - loss: 0.9789 - mean_absolute_error: 0.792780000/80000 [==============================] - 5s 60us/step - loss: 0.9697 - mean_absolute_error: 0.7883 - val_loss: 1.0732 - val_mean_absolute_error: 0.8341\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9304 - mean_absolute_error: 0.7675 - val_loss: 1.0445 - val_mean_absolute_error: 0.8176\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9002 - mean_absolute_error: 0.7528 - val_loss: 1.0216 - val_mean_absolute_error: 0.8060\n",
      "Epoch 11/150\n",
      "51936/80000 [==================>...........] - ETA: 1s - loss: 0.8714 - mean_absolute_error: 0.738980000/80000 [==============================] - 5s 60us/step - loss: 0.8730 - mean_absolute_error: 0.7392 - val_loss: 1.0020 - val_mean_absolute_error: 0.7959\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8491 - mean_absolute_error: 0.7277 - val_loss: 0.9869 - val_mean_absolute_error: 0.7881\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8313 - mean_absolute_error: 0.7187 - val_loss: 0.9754 - val_mean_absolute_error: 0.7817\n",
      "Epoch 14/150\n",
      "50592/80000 [=================>............] - ETA: 1s - loss: 0.8154 - mean_absolute_error: 0.710580000/80000 [==============================] - 5s 61us/step - loss: 0.8157 - mean_absolute_error: 0.7115 - val_loss: 0.9662 - val_mean_absolute_error: 0.7790\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8031 - mean_absolute_error: 0.7047 - val_loss: 0.9573 - val_mean_absolute_error: 0.7749\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7907 - mean_absolute_error: 0.6993 - val_loss: 0.9507 - val_mean_absolute_error: 0.7717\n",
      "Epoch 17/150\n",
      "50080/80000 [=================>............] - ETA: 1s - loss: 0.7749 - mean_absolute_error: 0.690380000/80000 [==============================] - 5s 60us/step - loss: 0.7789 - mean_absolute_error: 0.6930 - val_loss: 0.9457 - val_mean_absolute_error: 0.7680\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7671 - mean_absolute_error: 0.6873 - val_loss: 0.9420 - val_mean_absolute_error: 0.7664\n",
      "Epoch 19/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.7563 - mean_absolute_error: 0.6818 - val_loss: 0.9376 - val_mean_absolute_error: 0.7646\n",
      "Epoch 20/150\n",
      "46944/80000 [================>.............] - ETA: 1s - loss: 0.7513 - mean_absolute_error: 0.679580000/80000 [==============================] - 5s 61us/step - loss: 0.7540 - mean_absolute_error: 0.6806 - val_loss: 0.9344 - val_mean_absolute_error: 0.7627\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.7440 - mean_absolute_error: 0.6760 - val_loss: 0.9318 - val_mean_absolute_error: 0.7629\n",
      "Epoch 22/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7395 - mean_absolute_error: 0.6739 - val_loss: 0.9293 - val_mean_absolute_error: 0.7604\n",
      "Epoch 23/150\n",
      "47712/80000 [================>.............] - ETA: 1s - loss: 0.7289 - mean_absolute_error: 0.666980000/80000 [==============================] - 5s 61us/step - loss: 0.7329 - mean_absolute_error: 0.6696 - val_loss: 0.9276 - val_mean_absolute_error: 0.7595\n",
      "Epoch 24/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7255 - mean_absolute_error: 0.6672 - val_loss: 0.9263 - val_mean_absolute_error: 0.7588\n",
      "Epoch 25/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7175 - mean_absolute_error: 0.6628 - val_loss: 0.9249 - val_mean_absolute_error: 0.7590\n",
      "Epoch 26/150\n",
      "48544/80000 [=================>............] - ETA: 1s - loss: 0.7097 - mean_absolute_error: 0.659180000/80000 [==============================] - 5s 61us/step - loss: 0.7136 - mean_absolute_error: 0.6605 - val_loss: 0.9239 - val_mean_absolute_error: 0.7576\n",
      "Epoch 27/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.7107 - mean_absolute_error: 0.6591 - val_loss: 0.9227 - val_mean_absolute_error: 0.7565\n",
      "Epoch 28/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7029 - mean_absolute_error: 0.6558 - val_loss: 0.9204 - val_mean_absolute_error: 0.7566\n",
      "Epoch 29/150\n",
      "48800/80000 [=================>............] - ETA: 1s - loss: 0.6962 - mean_absolute_error: 0.652380000/80000 [==============================] - 5s 60us/step - loss: 0.6986 - mean_absolute_error: 0.6528 - val_loss: 0.9185 - val_mean_absolute_error: 0.7560\n",
      "Epoch 30/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.6969 - mean_absolute_error: 0.6522 - val_loss: 0.9179 - val_mean_absolute_error: 0.7547\n",
      "Epoch 31/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.6894 - mean_absolute_error: 0.6479 - val_loss: 0.9175 - val_mean_absolute_error: 0.7547\n",
      "Epoch 32/150\n",
      "50016/80000 [=================>............] - ETA: 1s - loss: 0.6840 - mean_absolute_error: 0.645780000/80000 [==============================] - 5s 60us/step - loss: 0.6899 - mean_absolute_error: 0.6491 - val_loss: 0.9169 - val_mean_absolute_error: 0.7546\n",
      "Epoch 33/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.6800 - mean_absolute_error: 0.6439 - val_loss: 0.9167 - val_mean_absolute_error: 0.7550\n",
      "Epoch 34/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.6807 - mean_absolute_error: 0.6440 - val_loss: 0.9171 - val_mean_absolute_error: 0.7541\n",
      "Epoch 35/150\n",
      "50176/80000 [=================>............] - ETA: 1s - loss: 0.6705 - mean_absolute_error: 0.638280000/80000 [==============================] - 5s 60us/step - loss: 0.6729 - mean_absolute_error: 0.6400 - val_loss: 0.9169 - val_mean_absolute_error: 0.7545\n",
      "Epoch 36/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.6722 - mean_absolute_error: 0.6394 - val_loss: 0.9167 - val_mean_absolute_error: 0.7537\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 4.6556 - mean_absolute_error: 1.7940 - val_loss: 1.3484 - val_mean_absolute_error: 0.9835\n",
      "Epoch 2/150\n",
      "47200/80000 [================>.............] - ETA: 1s - loss: 1.2450 - mean_absolute_error: 0.938680000/80000 [==============================] - 4s 56us/step - loss: 1.2424 - mean_absolute_error: 0.9364 - val_loss: 1.3114 - val_mean_absolute_error: 0.9586\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.1813 - mean_absolute_error: 0.9063 - val_loss: 1.2353 - val_mean_absolute_error: 0.9260\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.0839 - mean_absolute_error: 0.8536 - val_loss: 1.1566 - val_mean_absolute_error: 0.8820\n",
      "Epoch 5/150\n",
      "63712/80000 [======================>.......] - ETA: 0s - loss: 1.0210 - mean_absolute_error: 0.817380000/80000 [==============================] - 4s 56us/step - loss: 1.0165 - mean_absolute_error: 0.8150 - val_loss: 1.1038 - val_mean_absolute_error: 0.8499\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9602 - mean_absolute_error: 0.7830 - val_loss: 1.0613 - val_mean_absolute_error: 0.8255\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9095 - mean_absolute_error: 0.7565 - val_loss: 1.0256 - val_mean_absolute_error: 0.8059\n",
      "Epoch 8/150\n",
      "65088/80000 [=======================>......] - ETA: 0s - loss: 0.8664 - mean_absolute_error: 0.736380000/80000 [==============================] - 4s 56us/step - loss: 0.8653 - mean_absolute_error: 0.7357 - val_loss: 0.9987 - val_mean_absolute_error: 0.7916\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.8305 - mean_absolute_error: 0.7190 - val_loss: 0.9798 - val_mean_absolute_error: 0.7819\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.8007 - mean_absolute_error: 0.7047 - val_loss: 0.9660 - val_mean_absolute_error: 0.7758\n",
      "Epoch 11/150\n",
      "65280/80000 [=======================>......] - ETA: 0s - loss: 0.7793 - mean_absolute_error: 0.693680000/80000 [==============================] - 4s 56us/step - loss: 0.7781 - mean_absolute_error: 0.6931 - val_loss: 0.9563 - val_mean_absolute_error: 0.7701\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7554 - mean_absolute_error: 0.6820 - val_loss: 0.9497 - val_mean_absolute_error: 0.7671\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.7355 - mean_absolute_error: 0.6724 - val_loss: 0.9445 - val_mean_absolute_error: 0.7654\n",
      "Epoch 14/150\n",
      "65696/80000 [=======================>......] - ETA: 0s - loss: 0.7207 - mean_absolute_error: 0.664380000/80000 [==============================] - 4s 56us/step - loss: 0.7181 - mean_absolute_error: 0.6635 - val_loss: 0.9407 - val_mean_absolute_error: 0.7630\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.7035 - mean_absolute_error: 0.6561 - val_loss: 0.9385 - val_mean_absolute_error: 0.7627\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.6896 - mean_absolute_error: 0.6484 - val_loss: 0.9368 - val_mean_absolute_error: 0.7612\n",
      "Epoch 17/150\n",
      "66816/80000 [========================>.....] - ETA: 0s - loss: 0.6730 - mean_absolute_error: 0.640580000/80000 [==============================] - 4s 55us/step - loss: 0.6752 - mean_absolute_error: 0.6410 - val_loss: 0.9362 - val_mean_absolute_error: 0.7603\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.6642 - mean_absolute_error: 0.6355 - val_loss: 0.9360 - val_mean_absolute_error: 0.7596\n",
      "Epoch 19/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.6558 - mean_absolute_error: 0.6312 - val_loss: 0.9363 - val_mean_absolute_error: 0.7591\n",
      "Epoch 20/150\n",
      "65856/80000 [=======================>......] - ETA: 0s - loss: 0.6397 - mean_absolute_error: 0.622180000/80000 [==============================] - 4s 55us/step - loss: 0.6449 - mean_absolute_error: 0.6239 - val_loss: 0.9369 - val_mean_absolute_error: 0.7604\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.6368 - mean_absolute_error: 0.6198 - val_loss: 0.9377 - val_mean_absolute_error: 0.7603\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 4.7170 - mean_absolute_error: 1.8079 - val_loss: 1.3527 - val_mean_absolute_error: 0.9854\n",
      "Epoch 2/150\n",
      "43168/80000 [===============>..............] - ETA: 2s - loss: 1.2476 - mean_absolute_error: 0.939480000/80000 [==============================] - 5s 61us/step - loss: 1.2315 - mean_absolute_error: 0.9317 - val_loss: 1.2891 - val_mean_absolute_error: 0.9517\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 1.1371 - mean_absolute_error: 0.8832 - val_loss: 1.1854 - val_mean_absolute_error: 0.8985\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 63us/step - loss: 1.0430 - mean_absolute_error: 0.8294 - val_loss: 1.1174 - val_mean_absolute_error: 0.8583\n",
      "Epoch 5/150\n",
      "43488/80000 [===============>..............] - ETA: 2s - loss: 0.9890 - mean_absolute_error: 0.797680000/80000 [==============================] - 5s 63us/step - loss: 0.9813 - mean_absolute_error: 0.7936 - val_loss: 1.0716 - val_mean_absolute_error: 0.8308\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9257 - mean_absolute_error: 0.7641 - val_loss: 1.0330 - val_mean_absolute_error: 0.8097\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.8714 - mean_absolute_error: 0.7377 - val_loss: 1.0002 - val_mean_absolute_error: 0.7926\n",
      "Epoch 8/150\n",
      "46304/80000 [================>.............] - ETA: 1s - loss: 0.8280 - mean_absolute_error: 0.716580000/80000 [==============================] - 5s 61us/step - loss: 0.8243 - mean_absolute_error: 0.7153 - val_loss: 0.9762 - val_mean_absolute_error: 0.7814\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7834 - mean_absolute_error: 0.6958 - val_loss: 0.9587 - val_mean_absolute_error: 0.7726\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.7493 - mean_absolute_error: 0.6794 - val_loss: 0.9464 - val_mean_absolute_error: 0.7668\n",
      "Epoch 11/150\n",
      "47424/80000 [================>.............] - ETA: 1s - loss: 0.7115 - mean_absolute_error: 0.659680000/80000 [==============================] - 5s 61us/step - loss: 0.7174 - mean_absolute_error: 0.6631 - val_loss: 0.9379 - val_mean_absolute_error: 0.7632\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.6910 - mean_absolute_error: 0.6504 - val_loss: 0.9319 - val_mean_absolute_error: 0.7603\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.6692 - mean_absolute_error: 0.6382 - val_loss: 0.9282 - val_mean_absolute_error: 0.7574\n",
      "Epoch 14/150\n",
      "47328/80000 [================>.............] - ETA: 1s - loss: 0.6469 - mean_absolute_error: 0.627280000/80000 [==============================] - 5s 62us/step - loss: 0.6478 - mean_absolute_error: 0.6268 - val_loss: 0.9256 - val_mean_absolute_error: 0.7564\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 63us/step - loss: 0.6289 - mean_absolute_error: 0.6171 - val_loss: 0.9255 - val_mean_absolute_error: 0.7553\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.6098 - mean_absolute_error: 0.6072 - val_loss: 0.9255 - val_mean_absolute_error: 0.7560\n",
      "Epoch 17/150\n",
      "45856/80000 [================>.............] - ETA: 1s - loss: 0.5892 - mean_absolute_error: 0.596280000/80000 [==============================] - 5s 62us/step - loss: 0.5960 - mean_absolute_error: 0.5987 - val_loss: 0.9257 - val_mean_absolute_error: 0.7551\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.5797 - mean_absolute_error: 0.5899 - val_loss: 0.9271 - val_mean_absolute_error: 0.7549\n",
      "Epoch 19/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.5659 - mean_absolute_error: 0.5824 - val_loss: 0.9294 - val_mean_absolute_error: 0.7562\n"
     ]
    }
   ],
   "source": [
    "k_latents = [20, 30]\n",
    "dos = [0.5, 0.2]\n",
    "GMF_results = []\n",
    "for do in dos:\n",
    "  for k in k_latents:\n",
    "    GMF_model = get_gmf_model(max_userid,max_movieid,k,do)\n",
    "    GMF_model.compile(loss='mse',optimizer=Adamax(),metrics=['mae'])\n",
    "    callbacks = [EarlyStopping('val_loss', patience=3)]\n",
    "    GMF_history = GMF_model.fit([Users, Movies], Ratings, epochs=150, validation_split=.2, verbose=1, callbacks=callbacks, batch_size = 32)\n",
    "    GMF_results.append((k, do, GMF_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAKBBzk9Jcyd"
   },
   "source": [
    "**Average MAE for generalized matrix factorization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 701.0,
     "status": "ok",
     "timestamp": 1.532585106914E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "z-u5itrsJfWe",
    "outputId": "4c7e159b-8338-4462-a18c-d19baa8ab30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 latent dimension in 51 epochs and 0.5 drop out Average MAE: 0.7466326880896792 and Min MAE 0.6677943883299827\n",
      "Using 30 latent dimension in 42 epochs and 0.5 drop out Average MAE: 0.7344445350505057 and Min MAE 0.6331295590162277\n",
      "Using 20 latent dimension in 20 epochs and 0.2 drop out Average MAE: 0.7768717128753662 and Min MAE 0.6205486805081367\n",
      "Using 30 latent dimension in 19 epochs and 0.2 drop out Average MAE: 0.7597886738143469 and Min MAE 0.57933232640028\n"
     ]
    }
   ],
   "source": [
    "for k, do, history in GMF_results:\n",
    "  avg = np.mean(history.history['mean_absolute_error'])\n",
    "  epochs = len(history.history['mean_absolute_error'])\n",
    "  minimum = min(history.history['mean_absolute_error'])\n",
    "  print(\"Using\", k, \"latent dimension in\", epochs, \"epochs and\", do, \"drop out Average MAE:\", avg, \"and Min MAE\", minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBxphxPvMYnC"
   },
   "source": [
    "**Ex 1 Section C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhEUpAoWMfQt"
   },
   "source": [
    "Using Matrix Factorization, the average MAE was 0.84 with minimal MAE of 0.6166\n",
    "\n",
    "Using GMF's best run, the average MAE was 0.7597 with minimal MAE of 0.5793\n",
    "\n",
    "Ass 2 - the average MAE was 0.8272\n",
    "\n",
    "We saw that the time to train the new models - GMF and MF - are lonager than the model's train of ass 2 - minutes vs seconds.\n",
    "\n",
    "By the results of the min MAE, we can say that the new models are better than ass 2 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Nxz6tjqjrI_"
   },
   "source": [
    "**Define neural collaborative filtering model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Euhy_Kg--Mo1"
   },
   "outputs": [],
   "source": [
    "def get_ncf_model(num_users, num_items, latent_dim, hidden_dim, do):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    conc = Concatenate()([user_latent, item_latent])\n",
    "    drop = Dropout(0.5)(conc)\n",
    "    hid1 = Dense(hidden_dim, activation='relu')(conc)\n",
    "    drop2  = Dropout(do)(hid1)\n",
    "    prediction = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'prediction')(drop2)\n",
    "    \n",
    "    \n",
    "    model = Model(input=[user_input, item_input], output=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5CO9_XwPrNZ"
   },
   "source": [
    "##Ex 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ULE-yQNRPzso"
   },
   "outputs": [],
   "source": [
    "def get_ncf_model(num_users, num_items, latent_dim, hidden_dim, do, num_of_hidden):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    conc = Concatenate()([user_latent, item_latent])\n",
    "    drop = Dropout(0.5)(conc)\n",
    "    hid1 = Dense(hidden_dim, activation='relu')(drop)\n",
    "    last_layer = hid1\n",
    "    for i in range(num_of_hidden - 1):\n",
    "      hid_n = Dense(hidden_dim, activation='relu')(last_layer)\n",
    "      last_layer = hid_n\n",
    "    drop2  = Dropout(do)(last_layer)\n",
    "    prediction = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'prediction')(drop2)\n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8027.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1121585.0,
     "status": "ok",
     "timestamp": 1.532810525249E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "CZpLH-TLP4HS",
    "outputId": "9aa39258-93cc-4e4b-e248-c076815889be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 6s 70us/step - loss: 2.6919 - mean_absolute_error: 1.3034 - val_loss: 1.1339 - val_mean_absolute_error: 0.8821\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 1.2526 - mean_absolute_error: 0.9045 - val_loss: 1.0109 - val_mean_absolute_error: 0.8165\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 1.0381 - mean_absolute_error: 0.8187 - val_loss: 0.9704 - val_mean_absolute_error: 0.7896\n",
      "Epoch 4/150\n",
      "14752/80000 [====>.........................] - ETA: 3s - loss: 0.9960 - mean_absolute_error: 0.799580000/80000 [==============================] - 5s 62us/step - loss: 0.9726 - mean_absolute_error: 0.7889 - val_loss: 0.9454 - val_mean_absolute_error: 0.7762\n",
      "Epoch 5/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9443 - mean_absolute_error: 0.7750 - val_loss: 0.9375 - val_mean_absolute_error: 0.7707\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9303 - mean_absolute_error: 0.7678 - val_loss: 0.9300 - val_mean_absolute_error: 0.7648\n",
      "Epoch 7/150\n",
      "45088/80000 [===============>..............] - ETA: 1s - loss: 0.9259 - mean_absolute_error: 0.763780000/80000 [==============================] - 5s 61us/step - loss: 0.9299 - mean_absolute_error: 0.7662 - val_loss: 0.9264 - val_mean_absolute_error: 0.7636\n",
      "Epoch 8/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9223 - mean_absolute_error: 0.7640 - val_loss: 0.9241 - val_mean_absolute_error: 0.7627\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9192 - mean_absolute_error: 0.7615 - val_loss: 0.9210 - val_mean_absolute_error: 0.7611\n",
      "Epoch 10/150\n",
      "48288/80000 [=================>............] - ETA: 1s - loss: 0.9189 - mean_absolute_error: 0.760880000/80000 [==============================] - 5s 62us/step - loss: 0.9179 - mean_absolute_error: 0.7604 - val_loss: 0.9185 - val_mean_absolute_error: 0.7577\n",
      "Epoch 11/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9121 - mean_absolute_error: 0.7583 - val_loss: 0.9161 - val_mean_absolute_error: 0.7580\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9121 - mean_absolute_error: 0.7588 - val_loss: 0.9202 - val_mean_absolute_error: 0.7596\n",
      "Epoch 13/150\n",
      "46720/80000 [================>.............] - ETA: 1s - loss: 0.9054 - mean_absolute_error: 0.754680000/80000 [==============================] - 5s 62us/step - loss: 0.9078 - mean_absolute_error: 0.7566 - val_loss: 0.9122 - val_mean_absolute_error: 0.7573\n",
      "Epoch 14/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9086 - mean_absolute_error: 0.7571 - val_loss: 0.9132 - val_mean_absolute_error: 0.7578\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9047 - mean_absolute_error: 0.7557 - val_loss: 0.9106 - val_mean_absolute_error: 0.7581\n",
      "Epoch 16/150\n",
      "48544/80000 [=================>............] - ETA: 1s - loss: 0.8962 - mean_absolute_error: 0.752180000/80000 [==============================] - 5s 61us/step - loss: 0.9015 - mean_absolute_error: 0.7543 - val_loss: 0.9079 - val_mean_absolute_error: 0.7531\n",
      "Epoch 17/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9013 - mean_absolute_error: 0.7535 - val_loss: 0.9075 - val_mean_absolute_error: 0.7550\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 63us/step - loss: 0.9010 - mean_absolute_error: 0.7535 - val_loss: 0.9083 - val_mean_absolute_error: 0.7546\n",
      "Epoch 19/150\n",
      "48000/80000 [=================>............] - ETA: 1s - loss: 0.8996 - mean_absolute_error: 0.753180000/80000 [==============================] - 5s 61us/step - loss: 0.9003 - mean_absolute_error: 0.7535 - val_loss: 0.9064 - val_mean_absolute_error: 0.7512\n",
      "Epoch 20/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9018 - mean_absolute_error: 0.7538 - val_loss: 0.9071 - val_mean_absolute_error: 0.7554\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8977 - mean_absolute_error: 0.7534 - val_loss: 0.9056 - val_mean_absolute_error: 0.7523\n",
      "Epoch 22/150\n",
      "51040/80000 [==================>...........] - ETA: 1s - loss: 0.8990 - mean_absolute_error: 0.753180000/80000 [==============================] - 5s 61us/step - loss: 0.8971 - mean_absolute_error: 0.7523 - val_loss: 0.9021 - val_mean_absolute_error: 0.7485\n",
      "Epoch 23/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8989 - mean_absolute_error: 0.7529 - val_loss: 0.8988 - val_mean_absolute_error: 0.7478\n",
      "Epoch 24/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8978 - mean_absolute_error: 0.7520 - val_loss: 0.9045 - val_mean_absolute_error: 0.7528\n",
      "Epoch 25/150\n",
      "52704/80000 [==================>...........] - ETA: 1s - loss: 0.8942 - mean_absolute_error: 0.750280000/80000 [==============================] - 5s 60us/step - loss: 0.8962 - mean_absolute_error: 0.7516 - val_loss: 0.9021 - val_mean_absolute_error: 0.7521\n",
      "Epoch 26/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8960 - mean_absolute_error: 0.7512 - val_loss: 0.9042 - val_mean_absolute_error: 0.7531\n",
      "Epoch 27/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8945 - mean_absolute_error: 0.7517 - val_loss: 0.9059 - val_mean_absolute_error: 0.7560\n",
      "Epoch 28/150\n",
      "52512/80000 [==================>...........] - ETA: 1s - loss: 0.8867 - mean_absolute_error: 0.746780000/80000 [==============================] - 5s 61us/step - loss: 0.8934 - mean_absolute_error: 0.7498 - val_loss: 0.8998 - val_mean_absolute_error: 0.7506\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 1.3589 - mean_absolute_error: 0.9589 - val_loss: 1.2723 - val_mean_absolute_error: 0.9427\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.1538 - mean_absolute_error: 0.8837 - val_loss: 1.1163 - val_mean_absolute_error: 0.8611\n",
      "Epoch 3/150\n",
      "51072/80000 [==================>...........] - ETA: 1s - loss: 1.0717 - mean_absolute_error: 0.838480000/80000 [==============================] - 4s 55us/step - loss: 1.0650 - mean_absolute_error: 0.8344 - val_loss: 1.0315 - val_mean_absolute_error: 0.8149\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 1.0208 - mean_absolute_error: 0.8128 - val_loss: 0.9911 - val_mean_absolute_error: 0.7944\n",
      "Epoch 5/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9976 - mean_absolute_error: 0.8013 - val_loss: 0.9759 - val_mean_absolute_error: 0.7896\n",
      "Epoch 6/150\n",
      "66432/80000 [=======================>......] - ETA: 0s - loss: 0.9896 - mean_absolute_error: 0.798680000/80000 [==============================] - 4s 55us/step - loss: 0.9870 - mean_absolute_error: 0.7970 - val_loss: 0.9662 - val_mean_absolute_error: 0.7861\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9771 - mean_absolute_error: 0.7924 - val_loss: 0.9577 - val_mean_absolute_error: 0.7847\n",
      "Epoch 8/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9634 - mean_absolute_error: 0.7867 - val_loss: 0.9424 - val_mean_absolute_error: 0.7753\n",
      "Epoch 9/150\n",
      "68896/80000 [========================>.....] - ETA: 0s - loss: 0.9583 - mean_absolute_error: 0.784280000/80000 [==============================] - 4s 56us/step - loss: 0.9567 - mean_absolute_error: 0.7832 - val_loss: 0.9357 - val_mean_absolute_error: 0.7696\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9488 - mean_absolute_error: 0.7792 - val_loss: 0.9320 - val_mean_absolute_error: 0.7694\n",
      "Epoch 11/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9478 - mean_absolute_error: 0.7790 - val_loss: 0.9306 - val_mean_absolute_error: 0.7682\n",
      "Epoch 12/150\n",
      "70784/80000 [=========================>....] - ETA: 0s - loss: 0.9419 - mean_absolute_error: 0.776580000/80000 [==============================] - 4s 55us/step - loss: 0.9409 - mean_absolute_error: 0.7763 - val_loss: 0.9211 - val_mean_absolute_error: 0.7604\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9369 - mean_absolute_error: 0.7737 - val_loss: 0.9285 - val_mean_absolute_error: 0.7654\n",
      "Epoch 14/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9353 - mean_absolute_error: 0.7734 - val_loss: 0.9238 - val_mean_absolute_error: 0.7659\n",
      "Epoch 15/150\n",
      "71712/80000 [=========================>....] - ETA: 0s - loss: 0.9312 - mean_absolute_error: 0.771880000/80000 [==============================] - 4s 55us/step - loss: 0.9342 - mean_absolute_error: 0.7726 - val_loss: 0.9237 - val_mean_absolute_error: 0.7653\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9281 - mean_absolute_error: 0.7694 - val_loss: 0.9166 - val_mean_absolute_error: 0.7643\n",
      "Epoch 17/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9275 - mean_absolute_error: 0.7694 - val_loss: 0.9164 - val_mean_absolute_error: 0.7627\n",
      "Epoch 18/150\n",
      "73280/80000 [==========================>...] - ETA: 0s - loss: 0.9272 - mean_absolute_error: 0.770180000/80000 [==============================] - 4s 55us/step - loss: 0.9259 - mean_absolute_error: 0.7697 - val_loss: 0.9204 - val_mean_absolute_error: 0.7695\n",
      "Epoch 19/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9274 - mean_absolute_error: 0.7696 - val_loss: 0.9125 - val_mean_absolute_error: 0.7611\n",
      "Epoch 20/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9255 - mean_absolute_error: 0.7689 - val_loss: 0.9092 - val_mean_absolute_error: 0.7602\n",
      "Epoch 21/150\n",
      "71488/80000 [=========================>....] - ETA: 0s - loss: 0.9253 - mean_absolute_error: 0.768480000/80000 [==============================] - 4s 56us/step - loss: 0.9226 - mean_absolute_error: 0.7673 - val_loss: 0.9209 - val_mean_absolute_error: 0.7678\n",
      "Epoch 22/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9204 - mean_absolute_error: 0.7669 - val_loss: 0.9072 - val_mean_absolute_error: 0.7599\n",
      "Epoch 23/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9202 - mean_absolute_error: 0.7667 - val_loss: 0.9061 - val_mean_absolute_error: 0.7586\n",
      "Epoch 24/150\n",
      "70336/80000 [=========================>....] - ETA: 0s - loss: 0.9178 - mean_absolute_error: 0.765380000/80000 [==============================] - 4s 56us/step - loss: 0.9187 - mean_absolute_error: 0.7660 - val_loss: 0.9104 - val_mean_absolute_error: 0.7594\n",
      "Epoch 25/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9173 - mean_absolute_error: 0.7649 - val_loss: 0.9114 - val_mean_absolute_error: 0.7589\n",
      "Epoch 26/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.9171 - mean_absolute_error: 0.7648 - val_loss: 0.9080 - val_mean_absolute_error: 0.7593\n",
      "Epoch 27/150\n",
      "70336/80000 [=========================>....] - ETA: 0s - loss: 0.9143 - mean_absolute_error: 0.763480000/80000 [==============================] - 4s 56us/step - loss: 0.9163 - mean_absolute_error: 0.7643 - val_loss: 0.9099 - val_mean_absolute_error: 0.7583\n",
      "Epoch 28/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9145 - mean_absolute_error: 0.7640 - val_loss: 0.9151 - val_mean_absolute_error: 0.7592\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 5s 67us/step - loss: 2.2967 - mean_absolute_error: 1.1919 - val_loss: 1.0788 - val_mean_absolute_error: 0.8536\n",
      "Epoch 2/150\n",
      "46496/80000 [================>.............] - ETA: 1s - loss: 1.3031 - mean_absolute_error: 0.920180000/80000 [==============================] - 5s 61us/step - loss: 1.2498 - mean_absolute_error: 0.9011 - val_loss: 0.9868 - val_mean_absolute_error: 0.8029\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 1.0563 - mean_absolute_error: 0.8257 - val_loss: 0.9714 - val_mean_absolute_error: 0.7977\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9796 - mean_absolute_error: 0.7932 - val_loss: 0.9407 - val_mean_absolute_error: 0.7719\n",
      "Epoch 5/150\n",
      "47392/80000 [================>.............] - ETA: 1s - loss: 0.9435 - mean_absolute_error: 0.776280000/80000 [==============================] - 5s 64us/step - loss: 0.9464 - mean_absolute_error: 0.7771 - val_loss: 0.9335 - val_mean_absolute_error: 0.7695\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9341 - mean_absolute_error: 0.7715 - val_loss: 0.9309 - val_mean_absolute_error: 0.7692\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9247 - mean_absolute_error: 0.7652 - val_loss: 0.9281 - val_mean_absolute_error: 0.7656\n",
      "Epoch 8/150\n",
      "48000/80000 [=================>............] - ETA: 1s - loss: 0.9145 - mean_absolute_error: 0.762180000/80000 [==============================] - 5s 62us/step - loss: 0.9221 - mean_absolute_error: 0.7652 - val_loss: 0.9225 - val_mean_absolute_error: 0.7640\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9178 - mean_absolute_error: 0.7621 - val_loss: 0.9244 - val_mean_absolute_error: 0.7662\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9140 - mean_absolute_error: 0.7608 - val_loss: 0.9194 - val_mean_absolute_error: 0.7601\n",
      "Epoch 11/150\n",
      "50592/80000 [=================>............] - ETA: 1s - loss: 0.9015 - mean_absolute_error: 0.755080000/80000 [==============================] - 5s 62us/step - loss: 0.9137 - mean_absolute_error: 0.7605 - val_loss: 0.9164 - val_mean_absolute_error: 0.7599\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9121 - mean_absolute_error: 0.7587 - val_loss: 0.9170 - val_mean_absolute_error: 0.7636\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9080 - mean_absolute_error: 0.7573 - val_loss: 0.9140 - val_mean_absolute_error: 0.7565\n",
      "Epoch 14/150\n",
      "50112/80000 [=================>............] - ETA: 1s - loss: 0.9001 - mean_absolute_error: 0.754080000/80000 [==============================] - 5s 62us/step - loss: 0.9056 - mean_absolute_error: 0.7557 - val_loss: 0.9156 - val_mean_absolute_error: 0.7587\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9014 - mean_absolute_error: 0.7542 - val_loss: 0.9140 - val_mean_absolute_error: 0.7587\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8982 - mean_absolute_error: 0.7529 - val_loss: 0.9078 - val_mean_absolute_error: 0.7519\n",
      "Epoch 17/150\n",
      "51584/80000 [==================>...........] - ETA: 1s - loss: 0.8996 - mean_absolute_error: 0.752580000/80000 [==============================] - 5s 61us/step - loss: 0.8963 - mean_absolute_error: 0.7513 - val_loss: 0.9083 - val_mean_absolute_error: 0.7553\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.8978 - mean_absolute_error: 0.7525 - val_loss: 0.9062 - val_mean_absolute_error: 0.7547\n",
      "Epoch 19/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8967 - mean_absolute_error: 0.7522 - val_loss: 0.9029 - val_mean_absolute_error: 0.7522\n",
      "Epoch 20/150\n",
      "50816/80000 [==================>...........] - ETA: 1s - loss: 0.8956 - mean_absolute_error: 0.750980000/80000 [==============================] - 5s 60us/step - loss: 0.8962 - mean_absolute_error: 0.7517 - val_loss: 0.9021 - val_mean_absolute_error: 0.7510\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8931 - mean_absolute_error: 0.7500 - val_loss: 0.9027 - val_mean_absolute_error: 0.7498\n",
      "Epoch 22/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8951 - mean_absolute_error: 0.7512 - val_loss: 0.9044 - val_mean_absolute_error: 0.7558\n",
      "Epoch 23/150\n",
      "52864/80000 [==================>...........] - ETA: 1s - loss: 0.8926 - mean_absolute_error: 0.749580000/80000 [==============================] - 5s 60us/step - loss: 0.8948 - mean_absolute_error: 0.7510 - val_loss: 0.9035 - val_mean_absolute_error: 0.7530\n",
      "Epoch 24/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8900 - mean_absolute_error: 0.7484 - val_loss: 0.9046 - val_mean_absolute_error: 0.7547\n",
      "Epoch 25/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8907 - mean_absolute_error: 0.7488 - val_loss: 0.9012 - val_mean_absolute_error: 0.7509\n",
      "Epoch 26/150\n",
      "52736/80000 [==================>...........] - ETA: 1s - loss: 0.8924 - mean_absolute_error: 0.749980000/80000 [==============================] - 5s 61us/step - loss: 0.8869 - mean_absolute_error: 0.7476 - val_loss: 0.9019 - val_mean_absolute_error: 0.7520\n",
      "Epoch 27/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8901 - mean_absolute_error: 0.7488 - val_loss: 0.8998 - val_mean_absolute_error: 0.7511\n",
      "Epoch 28/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8954 - mean_absolute_error: 0.7509 - val_loss: 0.9029 - val_mean_absolute_error: 0.7502\n",
      "Epoch 29/150\n",
      "53376/80000 [===================>..........] - ETA: 1s - loss: 0.8866 - mean_absolute_error: 0.746380000/80000 [==============================] - 5s 61us/step - loss: 0.8886 - mean_absolute_error: 0.7481 - val_loss: 0.9001 - val_mean_absolute_error: 0.7484\n",
      "Epoch 30/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8899 - mean_absolute_error: 0.7493 - val_loss: 0.8987 - val_mean_absolute_error: 0.7498\n",
      "Epoch 31/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.8863 - mean_absolute_error: 0.7479 - val_loss: 0.8997 - val_mean_absolute_error: 0.7499\n",
      "Epoch 32/150\n",
      "51488/80000 [==================>...........] - ETA: 1s - loss: 0.8828 - mean_absolute_error: 0.746780000/80000 [==============================] - 5s 62us/step - loss: 0.8867 - mean_absolute_error: 0.7481 - val_loss: 0.9011 - val_mean_absolute_error: 0.7517\n",
      "Epoch 33/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.8877 - mean_absolute_error: 0.7479 - val_loss: 0.8971 - val_mean_absolute_error: 0.7475\n",
      "Epoch 34/150\n",
      "80000/80000 [==============================] - 5s 63us/step - loss: 0.8863 - mean_absolute_error: 0.7474 - val_loss: 0.9001 - val_mean_absolute_error: 0.7506\n",
      "Epoch 35/150\n",
      "48160/80000 [=================>............] - ETA: 1s - loss: 0.8816 - mean_absolute_error: 0.745880000/80000 [==============================] - 5s 62us/step - loss: 0.8872 - mean_absolute_error: 0.7480 - val_loss: 0.8957 - val_mean_absolute_error: 0.7491\n",
      "Epoch 36/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.8861 - mean_absolute_error: 0.7469 - val_loss: 0.8990 - val_mean_absolute_error: 0.7480\n",
      "Epoch 37/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8900 - mean_absolute_error: 0.7492 - val_loss: 0.9008 - val_mean_absolute_error: 0.7539\n",
      "Epoch 38/150\n",
      "49632/80000 [=================>............] - ETA: 1s - loss: 0.8872 - mean_absolute_error: 0.749080000/80000 [==============================] - 5s 61us/step - loss: 0.8891 - mean_absolute_error: 0.7492 - val_loss: 0.9038 - val_mean_absolute_error: 0.7552\n",
      "Epoch 39/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8875 - mean_absolute_error: 0.7473 - val_loss: 0.8984 - val_mean_absolute_error: 0.7503\n",
      "Epoch 40/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8839 - mean_absolute_error: 0.7463 - val_loss: 0.8971 - val_mean_absolute_error: 0.7484\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "53440/80000 [===================>..........] - ETA: 1s - loss: 1.4194 - mean_absolute_error: 0.971680000/80000 [==============================] - 5s 62us/step - loss: 1.3511 - mean_absolute_error: 0.9539 - val_loss: 1.2525 - val_mean_absolute_error: 0.9337\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 1.1243 - mean_absolute_error: 0.8683 - val_loss: 1.0825 - val_mean_absolute_error: 0.8474\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 1.0414 - mean_absolute_error: 0.8220 - val_loss: 1.0045 - val_mean_absolute_error: 0.8053\n",
      "Epoch 4/150\n",
      "68096/80000 [========================>.....] - ETA: 0s - loss: 1.0005 - mean_absolute_error: 0.801080000/80000 [==============================] - 4s 56us/step - loss: 0.9980 - mean_absolute_error: 0.7997 - val_loss: 0.9738 - val_mean_absolute_error: 0.7841\n",
      "Epoch 5/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.9739 - mean_absolute_error: 0.7893 - val_loss: 0.9529 - val_mean_absolute_error: 0.7758\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9524 - mean_absolute_error: 0.7797 - val_loss: 0.9474 - val_mean_absolute_error: 0.7775\n",
      "Epoch 7/150\n",
      "68928/80000 [========================>.....] - ETA: 0s - loss: 0.9378 - mean_absolute_error: 0.772680000/80000 [==============================] - 5s 57us/step - loss: 0.9415 - mean_absolute_error: 0.7749 - val_loss: 0.9297 - val_mean_absolute_error: 0.7682\n",
      "Epoch 8/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9319 - mean_absolute_error: 0.7697 - val_loss: 0.9266 - val_mean_absolute_error: 0.7641\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.9273 - mean_absolute_error: 0.7680 - val_loss: 0.9213 - val_mean_absolute_error: 0.7619\n",
      "Epoch 10/150\n",
      "69760/80000 [=========================>....] - ETA: 0s - loss: 0.9183 - mean_absolute_error: 0.763280000/80000 [==============================] - 4s 56us/step - loss: 0.9203 - mean_absolute_error: 0.7642 - val_loss: 0.9159 - val_mean_absolute_error: 0.7547\n",
      "Epoch 11/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.9157 - mean_absolute_error: 0.7626 - val_loss: 0.9196 - val_mean_absolute_error: 0.7562\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.9124 - mean_absolute_error: 0.7613 - val_loss: 0.9184 - val_mean_absolute_error: 0.7573\n",
      "Epoch 13/150\n",
      "67072/80000 [========================>.....] - ETA: 0s - loss: 0.9049 - mean_absolute_error: 0.757480000/80000 [==============================] - 4s 56us/step - loss: 0.9078 - mean_absolute_error: 0.7593 - val_loss: 0.9110 - val_mean_absolute_error: 0.7530\n",
      "Epoch 14/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.9079 - mean_absolute_error: 0.7589 - val_loss: 0.9111 - val_mean_absolute_error: 0.7552\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 57us/step - loss: 0.9007 - mean_absolute_error: 0.7568 - val_loss: 0.9061 - val_mean_absolute_error: 0.7514\n",
      "Epoch 16/150\n",
      "67328/80000 [========================>.....] - ETA: 0s - loss: 0.9024 - mean_absolute_error: 0.757180000/80000 [==============================] - 5s 56us/step - loss: 0.9034 - mean_absolute_error: 0.7573 - val_loss: 0.9005 - val_mean_absolute_error: 0.7518\n",
      "Epoch 17/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.8985 - mean_absolute_error: 0.7536 - val_loss: 0.9027 - val_mean_absolute_error: 0.7532\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.8957 - mean_absolute_error: 0.7533 - val_loss: 0.9000 - val_mean_absolute_error: 0.7493\n",
      "Epoch 19/150\n",
      "66880/80000 [========================>.....] - ETA: 0s - loss: 0.8960 - mean_absolute_error: 0.753580000/80000 [==============================] - 5s 56us/step - loss: 0.8980 - mean_absolute_error: 0.7542 - val_loss: 0.9045 - val_mean_absolute_error: 0.7503\n",
      "Epoch 20/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.8943 - mean_absolute_error: 0.7523 - val_loss: 0.9028 - val_mean_absolute_error: 0.7554\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.8932 - mean_absolute_error: 0.7530 - val_loss: 0.9026 - val_mean_absolute_error: 0.7478\n",
      "Epoch 22/150\n",
      "67712/80000 [========================>.....] - ETA: 0s - loss: 0.8946 - mean_absolute_error: 0.753380000/80000 [==============================] - 5s 57us/step - loss: 0.8923 - mean_absolute_error: 0.7520 - val_loss: 0.9032 - val_mean_absolute_error: 0.7511\n",
      "Epoch 23/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.8904 - mean_absolute_error: 0.7506 - val_loss: 0.8985 - val_mean_absolute_error: 0.7493\n",
      "Epoch 24/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.8893 - mean_absolute_error: 0.7509 - val_loss: 0.8938 - val_mean_absolute_error: 0.7490\n",
      "Epoch 25/150\n",
      "67744/80000 [========================>.....] - ETA: 0s - loss: 0.8881 - mean_absolute_error: 0.749080000/80000 [==============================] - 4s 56us/step - loss: 0.8889 - mean_absolute_error: 0.7496 - val_loss: 0.8962 - val_mean_absolute_error: 0.7499\n",
      "Epoch 26/150\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.8905 - mean_absolute_error: 0.7512 - val_loss: 0.8983 - val_mean_absolute_error: 0.7528\n",
      "Epoch 27/150\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.8862 - mean_absolute_error: 0.7497 - val_loss: 0.8996 - val_mean_absolute_error: 0.7491\n",
      "Epoch 28/150\n",
      "70112/80000 [=========================>....] - ETA: 0s - loss: 0.8850 - mean_absolute_error: 0.749480000/80000 [==============================] - 4s 56us/step - loss: 0.8841 - mean_absolute_error: 0.7490 - val_loss: 0.8980 - val_mean_absolute_error: 0.7443\n",
      "Epoch 29/150\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.8817 - mean_absolute_error: 0.7460 - val_loss: 0.8990 - val_mean_absolute_error: 0.7511\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 6s 73us/step - loss: 2.1884 - mean_absolute_error: 1.1789 - val_loss: 1.0866 - val_mean_absolute_error: 0.8598\n",
      "Epoch 2/150\n",
      "40256/80000 [==============>...............] - ETA: 2s - loss: 1.3626 - mean_absolute_error: 0.942380000/80000 [==============================] - 5s 65us/step - loss: 1.2735 - mean_absolute_error: 0.9107 - val_loss: 0.9831 - val_mean_absolute_error: 0.8036\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 1.0419 - mean_absolute_error: 0.8219 - val_loss: 0.9625 - val_mean_absolute_error: 0.7952\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9716 - mean_absolute_error: 0.7897 - val_loss: 0.9358 - val_mean_absolute_error: 0.7682\n",
      "Epoch 5/150\n",
      "40160/80000 [==============>...............] - ETA: 2s - loss: 0.9427 - mean_absolute_error: 0.774380000/80000 [==============================] - 5s 66us/step - loss: 0.9518 - mean_absolute_error: 0.7785 - val_loss: 0.9365 - val_mean_absolute_error: 0.7713\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9417 - mean_absolute_error: 0.7735 - val_loss: 0.9281 - val_mean_absolute_error: 0.7649\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9334 - mean_absolute_error: 0.7695 - val_loss: 0.9212 - val_mean_absolute_error: 0.7663\n",
      "Epoch 8/150\n",
      "38880/80000 [=============>................] - ETA: 2s - loss: 0.9272 - mean_absolute_error: 0.766680000/80000 [==============================] - 5s 66us/step - loss: 0.9290 - mean_absolute_error: 0.7675 - val_loss: 0.9180 - val_mean_absolute_error: 0.7624\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.9231 - mean_absolute_error: 0.7644 - val_loss: 0.9109 - val_mean_absolute_error: 0.7512\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9200 - mean_absolute_error: 0.7627 - val_loss: 0.9149 - val_mean_absolute_error: 0.7609\n",
      "Epoch 11/150\n",
      "39872/80000 [=============>................] - ETA: 2s - loss: 0.9196 - mean_absolute_error: 0.761980000/80000 [==============================] - 5s 65us/step - loss: 0.9177 - mean_absolute_error: 0.7619 - val_loss: 0.9139 - val_mean_absolute_error: 0.7586\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9183 - mean_absolute_error: 0.7620 - val_loss: 0.9138 - val_mean_absolute_error: 0.7559\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.9110 - mean_absolute_error: 0.7576 - val_loss: 0.9067 - val_mean_absolute_error: 0.7504\n",
      "Epoch 14/150\n",
      "39616/80000 [=============>................] - ETA: 2s - loss: 0.9053 - mean_absolute_error: 0.754980000/80000 [==============================] - 5s 65us/step - loss: 0.9083 - mean_absolute_error: 0.7568 - val_loss: 0.9072 - val_mean_absolute_error: 0.7534\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9105 - mean_absolute_error: 0.7564 - val_loss: 0.9072 - val_mean_absolute_error: 0.7530\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.9027 - mean_absolute_error: 0.7540 - val_loss: 0.9006 - val_mean_absolute_error: 0.7482\n",
      "Epoch 17/150\n",
      "38528/80000 [=============>................] - ETA: 2s - loss: 0.9070 - mean_absolute_error: 0.754980000/80000 [==============================] - 5s 65us/step - loss: 0.9034 - mean_absolute_error: 0.7538 - val_loss: 0.9060 - val_mean_absolute_error: 0.7478\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9005 - mean_absolute_error: 0.7529 - val_loss: 0.9104 - val_mean_absolute_error: 0.7499\n",
      "Epoch 19/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9027 - mean_absolute_error: 0.7534 - val_loss: 0.9022 - val_mean_absolute_error: 0.7520\n",
      "Epoch 20/150\n",
      "39520/80000 [=============>................] - ETA: 2s - loss: 0.9025 - mean_absolute_error: 0.754080000/80000 [==============================] - 5s 64us/step - loss: 0.8988 - mean_absolute_error: 0.7523 - val_loss: 0.9012 - val_mean_absolute_error: 0.7511\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 5s 64us/step - loss: 0.8979 - mean_absolute_error: 0.7515 - val_loss: 0.9008 - val_mean_absolute_error: 0.7473\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 5s 68us/step - loss: 1.3904 - mean_absolute_error: 0.9718 - val_loss: 1.3305 - val_mean_absolute_error: 0.9685\n",
      "Epoch 2/150\n",
      "39424/80000 [=============>................] - ETA: 2s - loss: 1.2525 - mean_absolute_error: 0.939780000/80000 [==============================] - 5s 59us/step - loss: 1.2508 - mean_absolute_error: 0.9387 - val_loss: 1.3279 - val_mean_absolute_error: 0.9664\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 1.2389 - mean_absolute_error: 0.9328 - val_loss: 1.2705 - val_mean_absolute_error: 0.9469\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 1.1080 - mean_absolute_error: 0.8565 - val_loss: 1.0377 - val_mean_absolute_error: 0.8296\n",
      "Epoch 5/150\n",
      "52640/80000 [==================>...........] - ETA: 1s - loss: 1.0129 - mean_absolute_error: 0.809480000/80000 [==============================] - 5s 59us/step - loss: 1.0115 - mean_absolute_error: 0.8084 - val_loss: 0.9815 - val_mean_absolute_error: 0.7915\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9854 - mean_absolute_error: 0.7963 - val_loss: 0.9590 - val_mean_absolute_error: 0.7891\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9694 - mean_absolute_error: 0.7888 - val_loss: 0.9466 - val_mean_absolute_error: 0.7773\n",
      "Epoch 8/150\n",
      "56640/80000 [====================>.........] - ETA: 1s - loss: 0.9484 - mean_absolute_error: 0.779180000/80000 [==============================] - 5s 60us/step - loss: 0.9546 - mean_absolute_error: 0.7819 - val_loss: 0.9410 - val_mean_absolute_error: 0.7712\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9459 - mean_absolute_error: 0.7778 - val_loss: 0.9321 - val_mean_absolute_error: 0.7724\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9437 - mean_absolute_error: 0.7760 - val_loss: 0.9283 - val_mean_absolute_error: 0.7627\n",
      "Epoch 11/150\n",
      "55648/80000 [===================>..........] - ETA: 1s - loss: 0.9360 - mean_absolute_error: 0.772780000/80000 [==============================] - 5s 59us/step - loss: 0.9398 - mean_absolute_error: 0.7739 - val_loss: 0.9337 - val_mean_absolute_error: 0.7690\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9342 - mean_absolute_error: 0.7706 - val_loss: 0.9402 - val_mean_absolute_error: 0.7622\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9324 - mean_absolute_error: 0.7694 - val_loss: 0.9182 - val_mean_absolute_error: 0.7554\n",
      "Epoch 14/150\n",
      "55648/80000 [===================>..........] - ETA: 1s - loss: 0.9204 - mean_absolute_error: 0.763080000/80000 [==============================] - 5s 61us/step - loss: 0.9244 - mean_absolute_error: 0.7657 - val_loss: 0.9186 - val_mean_absolute_error: 0.7599\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9200 - mean_absolute_error: 0.7638 - val_loss: 0.9089 - val_mean_absolute_error: 0.7529\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9185 - mean_absolute_error: 0.7633 - val_loss: 0.9208 - val_mean_absolute_error: 0.7564\n",
      "Epoch 17/150\n",
      "55072/80000 [===================>..........] - ETA: 1s - loss: 0.9154 - mean_absolute_error: 0.761480000/80000 [==============================] - 5s 60us/step - loss: 0.9130 - mean_absolute_error: 0.7606 - val_loss: 0.9053 - val_mean_absolute_error: 0.7549\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9095 - mean_absolute_error: 0.7596 - val_loss: 0.9031 - val_mean_absolute_error: 0.7516\n",
      "Epoch 19/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9054 - mean_absolute_error: 0.7573 - val_loss: 0.9043 - val_mean_absolute_error: 0.7555\n",
      "Epoch 20/150\n",
      "55840/80000 [===================>..........] - ETA: 1s - loss: 0.9080 - mean_absolute_error: 0.758980000/80000 [==============================] - 5s 60us/step - loss: 0.9052 - mean_absolute_error: 0.7576 - val_loss: 0.9029 - val_mean_absolute_error: 0.7507\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9043 - mean_absolute_error: 0.7577 - val_loss: 0.9100 - val_mean_absolute_error: 0.7582\n",
      "Epoch 22/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8983 - mean_absolute_error: 0.7544 - val_loss: 0.8961 - val_mean_absolute_error: 0.7501\n",
      "Epoch 23/150\n",
      "54624/80000 [===================>..........] - ETA: 1s - loss: 0.8944 - mean_absolute_error: 0.751680000/80000 [==============================] - 5s 60us/step - loss: 0.8963 - mean_absolute_error: 0.7529 - val_loss: 0.8991 - val_mean_absolute_error: 0.7482\n",
      "Epoch 24/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8979 - mean_absolute_error: 0.7544 - val_loss: 0.8968 - val_mean_absolute_error: 0.7501\n",
      "Epoch 25/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.8940 - mean_absolute_error: 0.7528 - val_loss: 0.8964 - val_mean_absolute_error: 0.7460\n",
      "Epoch 26/150\n",
      "55872/80000 [===================>..........] - ETA: 1s - loss: 0.8882 - mean_absolute_error: 0.750280000/80000 [==============================] - 5s 60us/step - loss: 0.8960 - mean_absolute_error: 0.7537 - val_loss: 0.8962 - val_mean_absolute_error: 0.7498\n",
      "Epoch 27/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8917 - mean_absolute_error: 0.7512 - val_loss: 0.8967 - val_mean_absolute_error: 0.7495\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 6s 76us/step - loss: 2.4941 - mean_absolute_error: 1.2587 - val_loss: 1.1260 - val_mean_absolute_error: 0.8780\n",
      "Epoch 2/150\n",
      "32832/80000 [===========>..................] - ETA: 2s - loss: 1.4168 - mean_absolute_error: 0.964980000/80000 [==============================] - 5s 66us/step - loss: 1.2652 - mean_absolute_error: 0.9083 - val_loss: 0.9985 - val_mean_absolute_error: 0.8170\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 1.0081 - mean_absolute_error: 0.8082 - val_loss: 0.9441 - val_mean_absolute_error: 0.7765\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.9546 - mean_absolute_error: 0.7826 - val_loss: 0.9372 - val_mean_absolute_error: 0.7673\n",
      "Epoch 5/150\n",
      "36256/80000 [============>.................] - ETA: 2s - loss: 0.9306 - mean_absolute_error: 0.770180000/80000 [==============================] - 5s 66us/step - loss: 0.9339 - mean_absolute_error: 0.7715 - val_loss: 0.9302 - val_mean_absolute_error: 0.7687\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.9264 - mean_absolute_error: 0.7668 - val_loss: 0.9243 - val_mean_absolute_error: 0.7622\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.9181 - mean_absolute_error: 0.7616 - val_loss: 0.9138 - val_mean_absolute_error: 0.7562\n",
      "Epoch 8/150\n",
      "38368/80000 [=============>................] - ETA: 2s - loss: 0.9193 - mean_absolute_error: 0.761780000/80000 [==============================] - 5s 65us/step - loss: 0.9111 - mean_absolute_error: 0.7577 - val_loss: 0.9118 - val_mean_absolute_error: 0.7543\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 0.9070 - mean_absolute_error: 0.7565 - val_loss: 0.9099 - val_mean_absolute_error: 0.7553\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.9017 - mean_absolute_error: 0.7545 - val_loss: 0.9094 - val_mean_absolute_error: 0.7508\n",
      "Epoch 11/150\n",
      "38560/80000 [=============>................] - ETA: 2s - loss: 0.9005 - mean_absolute_error: 0.752480000/80000 [==============================] - 5s 66us/step - loss: 0.8997 - mean_absolute_error: 0.7525 - val_loss: 0.8987 - val_mean_absolute_error: 0.7465\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 67us/step - loss: 0.8960 - mean_absolute_error: 0.7508 - val_loss: 0.9032 - val_mean_absolute_error: 0.7534\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.8934 - mean_absolute_error: 0.7496 - val_loss: 0.9024 - val_mean_absolute_error: 0.7521\n",
      "Epoch 14/150\n",
      "37216/80000 [============>.................] - ETA: 2s - loss: 0.8907 - mean_absolute_error: 0.745580000/80000 [==============================] - 5s 66us/step - loss: 0.8919 - mean_absolute_error: 0.7477 - val_loss: 0.9000 - val_mean_absolute_error: 0.7471\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.8857 - mean_absolute_error: 0.7460 - val_loss: 0.9027 - val_mean_absolute_error: 0.7536\n",
      "Epoch 16/150\n",
      "80000/80000 [==============================] - 5s 66us/step - loss: 0.8888 - mean_absolute_error: 0.7475 - val_loss: 0.9005 - val_mean_absolute_error: 0.7521\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "37792/80000 [=============>................] - ETA: 2s - loss: 1.5063 - mean_absolute_error: 0.996180000/80000 [==============================] - 6s 69us/step - loss: 1.3618 - mean_absolute_error: 0.9590 - val_loss: 1.2574 - val_mean_absolute_error: 0.9423\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 1.1155 - mean_absolute_error: 0.8567 - val_loss: 1.0707 - val_mean_absolute_error: 0.8326\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 1.0421 - mean_absolute_error: 0.8136 - val_loss: 1.0310 - val_mean_absolute_error: 0.8127\n",
      "Epoch 4/150\n",
      "55200/80000 [===================>..........] - ETA: 1s - loss: 1.0068 - mean_absolute_error: 0.799680000/80000 [==============================] - 5s 59us/step - loss: 1.0068 - mean_absolute_error: 0.8004 - val_loss: 0.9948 - val_mean_absolute_error: 0.7924\n",
      "Epoch 5/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.9844 - mean_absolute_error: 0.7919 - val_loss: 0.9600 - val_mean_absolute_error: 0.7784\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9678 - mean_absolute_error: 0.7855 - val_loss: 0.9585 - val_mean_absolute_error: 0.7832\n",
      "Epoch 7/150\n",
      "55552/80000 [===================>..........] - ETA: 1s - loss: 0.9621 - mean_absolute_error: 0.782680000/80000 [==============================] - 5s 60us/step - loss: 0.9603 - mean_absolute_error: 0.7819 - val_loss: 0.9450 - val_mean_absolute_error: 0.7703\n",
      "Epoch 8/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9520 - mean_absolute_error: 0.7780 - val_loss: 0.9444 - val_mean_absolute_error: 0.7663\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9479 - mean_absolute_error: 0.7754 - val_loss: 0.9353 - val_mean_absolute_error: 0.7640\n",
      "Epoch 10/150\n",
      "56576/80000 [====================>.........] - ETA: 1s - loss: 0.9470 - mean_absolute_error: 0.775680000/80000 [==============================] - 5s 60us/step - loss: 0.9450 - mean_absolute_error: 0.7751 - val_loss: 0.9380 - val_mean_absolute_error: 0.7676\n",
      "Epoch 11/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9395 - mean_absolute_error: 0.7730 - val_loss: 0.9361 - val_mean_absolute_error: 0.7634\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9398 - mean_absolute_error: 0.7714 - val_loss: 0.9273 - val_mean_absolute_error: 0.7625\n",
      "Epoch 13/150\n",
      "55136/80000 [===================>..........] - ETA: 1s - loss: 0.9277 - mean_absolute_error: 0.767480000/80000 [==============================] - 5s 61us/step - loss: 0.9306 - mean_absolute_error: 0.7683 - val_loss: 0.9215 - val_mean_absolute_error: 0.7619\n",
      "Epoch 14/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9281 - mean_absolute_error: 0.7670 - val_loss: 0.9168 - val_mean_absolute_error: 0.7585\n",
      "Epoch 15/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9298 - mean_absolute_error: 0.7675 - val_loss: 0.9259 - val_mean_absolute_error: 0.7616\n",
      "Epoch 16/150\n",
      "55136/80000 [===================>..........] - ETA: 1s - loss: 0.9265 - mean_absolute_error: 0.765180000/80000 [==============================] - 5s 60us/step - loss: 0.9258 - mean_absolute_error: 0.7658 - val_loss: 0.9234 - val_mean_absolute_error: 0.7611\n",
      "Epoch 17/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9280 - mean_absolute_error: 0.7664 - val_loss: 0.9190 - val_mean_absolute_error: 0.7554\n",
      "Epoch 18/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9229 - mean_absolute_error: 0.7642 - val_loss: 0.9108 - val_mean_absolute_error: 0.7532\n",
      "Epoch 19/150\n",
      "53888/80000 [===================>..........] - ETA: 1s - loss: 0.9201 - mean_absolute_error: 0.762380000/80000 [==============================] - 5s 61us/step - loss: 0.9217 - mean_absolute_error: 0.7636 - val_loss: 0.9134 - val_mean_absolute_error: 0.7548\n",
      "Epoch 20/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9189 - mean_absolute_error: 0.7627 - val_loss: 0.9133 - val_mean_absolute_error: 0.7620\n",
      "Epoch 21/150\n",
      "80000/80000 [==============================] - 5s 62us/step - loss: 0.9196 - mean_absolute_error: 0.7632 - val_loss: 0.9120 - val_mean_absolute_error: 0.7533\n",
      "Epoch 22/150\n",
      "50880/80000 [==================>...........] - ETA: 1s - loss: 0.9170 - mean_absolute_error: 0.760680000/80000 [==============================] - 5s 62us/step - loss: 0.9157 - mean_absolute_error: 0.7618 - val_loss: 0.9094 - val_mean_absolute_error: 0.7562\n",
      "Epoch 23/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9145 - mean_absolute_error: 0.7610 - val_loss: 0.9092 - val_mean_absolute_error: 0.7531\n",
      "Epoch 24/150\n",
      "80000/80000 [==============================] - 5s 61us/step - loss: 0.9133 - mean_absolute_error: 0.7605 - val_loss: 0.9115 - val_mean_absolute_error: 0.7521\n",
      "Epoch 25/150\n",
      "53280/80000 [==================>...........] - ETA: 1s - loss: 0.9146 - mean_absolute_error: 0.760980000/80000 [==============================] - 5s 60us/step - loss: 0.9158 - mean_absolute_error: 0.7614 - val_loss: 0.9138 - val_mean_absolute_error: 0.7582\n",
      "Epoch 26/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9139 - mean_absolute_error: 0.7604 - val_loss: 0.9059 - val_mean_absolute_error: 0.7545\n",
      "Epoch 27/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9128 - mean_absolute_error: 0.7607 - val_loss: 0.9114 - val_mean_absolute_error: 0.7585\n",
      "Epoch 28/150\n",
      "55680/80000 [===================>..........] - ETA: 1s - loss: 0.9115 - mean_absolute_error: 0.759480000/80000 [==============================] - 5s 60us/step - loss: 0.9113 - mean_absolute_error: 0.7597 - val_loss: 0.9094 - val_mean_absolute_error: 0.7572\n",
      "Epoch 29/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9091 - mean_absolute_error: 0.7588 - val_loss: 0.9075 - val_mean_absolute_error: 0.7560\n",
      "Epoch 30/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9064 - mean_absolute_error: 0.7578 - val_loss: 0.9085 - val_mean_absolute_error: 0.7512\n",
      "Epoch 31/150\n",
      "54464/80000 [===================>..........] - ETA: 1s - loss: 0.9086 - mean_absolute_error: 0.756780000/80000 [==============================] - 5s 60us/step - loss: 0.9076 - mean_absolute_error: 0.7569 - val_loss: 0.9057 - val_mean_absolute_error: 0.7525\n",
      "Epoch 32/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9087 - mean_absolute_error: 0.7585 - val_loss: 0.9077 - val_mean_absolute_error: 0.7633\n",
      "Epoch 33/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.9072 - mean_absolute_error: 0.7574 - val_loss: 0.9058 - val_mean_absolute_error: 0.7522\n",
      "Epoch 34/150\n",
      "56864/80000 [====================>.........] - ETA: 1s - loss: 0.9057 - mean_absolute_error: 0.756180000/80000 [==============================] - 5s 59us/step - loss: 0.9079 - mean_absolute_error: 0.7576 - val_loss: 0.9143 - val_mean_absolute_error: 0.7599\n",
      "Epoch 35/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.9051 - mean_absolute_error: 0.7564 - val_loss: 0.9068 - val_mean_absolute_error: 0.7598\n",
      "Epoch 36/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.9047 - mean_absolute_error: 0.7567 - val_loss: 0.9046 - val_mean_absolute_error: 0.7505\n",
      "Epoch 37/150\n",
      "58016/80000 [====================>.........] - ETA: 1s - loss: 0.9010 - mean_absolute_error: 0.756180000/80000 [==============================] - 5s 59us/step - loss: 0.9014 - mean_absolute_error: 0.7554 - val_loss: 0.8926 - val_mean_absolute_error: 0.7497\n",
      "Epoch 38/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9026 - mean_absolute_error: 0.7558 - val_loss: 0.9087 - val_mean_absolute_error: 0.7509\n",
      "Epoch 39/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.9009 - mean_absolute_error: 0.7549 - val_loss: 0.9027 - val_mean_absolute_error: 0.7500\n",
      "Epoch 40/150\n",
      "57024/80000 [====================>.........] - ETA: 1s - loss: 0.8931 - mean_absolute_error: 0.752380000/80000 [==============================] - 5s 59us/step - loss: 0.8990 - mean_absolute_error: 0.7540 - val_loss: 0.9034 - val_mean_absolute_error: 0.7531\n",
      "Epoch 41/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.8999 - mean_absolute_error: 0.7536 - val_loss: 0.9013 - val_mean_absolute_error: 0.7523\n",
      "Epoch 42/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.9025 - mean_absolute_error: 0.7557 - val_loss: 0.8969 - val_mean_absolute_error: 0.7492\n"
     ]
    }
   ],
   "source": [
    "K_LATENT = 20\n",
    "do = 0.5\n",
    "num_of_nuerons = [15, 20]\n",
    "num_of_layers = [1, 3]\n",
    "optimizers = [(Adamax(), 'Adamax'), (SGD(), 'SGD')]\n",
    "results = []\n",
    "for layers in num_of_layers:\n",
    "  for nuerons in num_of_nuerons:\n",
    "    for opt, opt_name in optimizers:\n",
    "      NCF_model = get_ncf_model(max_userid, max_movieid, K_LATENT, nuerons, do, layers)\n",
    "      NCF_model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "      callbacks_ncf = [EarlyStopping('val_loss', patience=5)]\n",
    "      NCF_history = NCF_model.fit([Users, Movies], Ratings, epochs=150, validation_split=.2, verbose=1, callbacks=callbacks_ncf, batch_size = 32)\n",
    "      results.append((layers, nuerons, opt_name, NCF_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 170.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 571.0,
     "status": "ok",
     "timestamp": 1.532810528256E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "N040QuEsP96-",
    "outputId": "ba200686-2684-48c0-a621-ae9ec908c106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Const parameters: Loss Function - MSE\n",
      "Using 1 hidden layers sized 15 , Adamax as optimizer in 28 epochs: Average MAE is 0.7850146589381355 Min MAE is 0.749778125333786\n",
      "Using 1 hidden layers sized 15 , SGD as optimizer in 28 epochs: Average MAE is 0.7884379261136056 Min MAE is 0.7639994487762451\n",
      "Using 1 hidden layers sized 20 , Adamax as optimizer in 40 epochs: Average MAE is 0.7708266069939732 Min MAE is 0.7462981087684631\n",
      "Using 1 hidden layers sized 20 , SGD as optimizer in 29 epochs: Average MAE is 0.7728053828054462 Min MAE is 0.7460360916137695\n",
      "Using 3 hidden layers sized 15 , Adamax as optimizer in 21 epochs: Average MAE is 0.7919087006943565 Min MAE is 0.7515310255050659\n",
      "Using 3 hidden layers sized 15 , SGD as optimizer in 27 epochs: Average MAE is 0.7906686696158516 Min MAE is 0.7512444159984588\n",
      "Using 3 hidden layers sized 20 , Adamax as optimizer in 16 epochs: Average MAE is 0.8012737548299134 Min MAE is 0.7460245637416839\n",
      "Using 3 hidden layers sized 20 , SGD as optimizer in 42 epochs: Average MAE is 0.772993517169782 Min MAE is 0.7535679046154022\n"
     ]
    }
   ],
   "source": [
    "print(\"Const parameters: Loss Function - MSE\")\n",
    "for num_of_layers, num_of_nuerons, optimizer, history in results:\n",
    "  avg = np.mean(history.history['mean_absolute_error'])\n",
    "  epochs = len(history.history['mean_absolute_error'])\n",
    "  minimum = min(history.history['mean_absolute_error'])\n",
    "  print(\"Using\", num_of_layers, \"hidden layers sized\", num_of_nuerons, \",\" , optimizer, \"as optimizer in\", epochs, \"epochs: Average MAE is\", avg,\"Min MAE is\", minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqmX1105mMWo"
   },
   "source": [
    "The minimal MAE (0.7404) for Neural Collaborative Filtering was achieved using 3 hidden layers sized 20, \"Adamax\" as optimizer and took 31 epochs to converge. Comparing to GMF, the minimal MAE of NCF is higher but its average MAE is lower. the best method the smallest MAE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ludOelb2pTKx"
   },
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbQOMz97ph6r"
   },
   "source": [
    "**Section A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be1pv5R4plem"
   },
   "source": [
    "Categorical feature (movie category) requires more effort in order to trasform than binary (gender) or continuos feature (age).\n",
    "Hence, we decided to add to the model with the best results so far (GMF), the features of age and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "l4gwbDWHpY_x"
   },
   "outputs": [],
   "source": [
    "u_cols = ['userid', 'age', 'gender']\n",
    "users_df = pd.read_csv(USERS_DATA_FILE_PATH, delimiter='|', encoding='latin-1', names=u_cols, usecols=[0, 1, 2])\n",
    "users_df['gender'].replace('M', 0, inplace=True)\n",
    "users_df['gender'].replace('F', 1, inplace=True)\n",
    "joined_with_users_df = pd.merge(joined_df, users_df,on='userid')\n",
    "Gender = joined_with_users_df['gender'].values\n",
    "Age = joined_with_users_df['age'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "31LkcFMIse-n"
   },
   "outputs": [],
   "source": [
    "def get_gmf_model_gen(num_users, num_items, latent_dim, do, max_age):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    gender_input = Input(shape=(1,), dtype='float32', name = 'gender_input')\n",
    "    age_input = Input(shape=(1,), dtype='float32', name = 'age_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)\n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    hidden1 = Multiply()([user_latent, item_latent])\n",
    "    conc = Concatenate()([hidden1, gender_input, age_input])\n",
    "    drop = Dropout(do)(conc)\n",
    "    prediction = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'prediction')(conc)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input,gender_input, age_input], output=prediction)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xQzwMrp9HZ1"
   },
   "source": [
    "**Section B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1006.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 132424.0,
     "status": "ok",
     "timestamp": 1.532810703146E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "ozxgKgIbsi91",
    "outputId": "b95f804f-7a78-4af7-eeba-57a3bb2c3a80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"pr...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 6s 74us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 5s 58us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 58us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 4/150\n",
      "21216/80000 [======>.......................] - ETA: 3s - loss: 13.7179 - mean_absolute_error: 3.530680000/80000 [==============================] - 5s 59us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 6s 73us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 5s 63us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 3/150\n",
      "24672/80000 [========>.....................] - ETA: 3s - loss: 13.8288 - mean_absolute_error: 3.548580000/80000 [==============================] - 5s 64us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 64us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 6s 70us/step - loss: 2.0762 - mean_absolute_error: 1.1775 - val_loss: 1.7841 - val_mean_absolute_error: 1.0791\n",
      "Epoch 2/150\n",
      "36608/80000 [============>.................] - ETA: 2s - loss: 1.5600 - mean_absolute_error: 1.018080000/80000 [==============================] - 5s 60us/step - loss: 1.4692 - mean_absolute_error: 0.9886 - val_loss: 1.4032 - val_mean_absolute_error: 0.9611\n",
      "Epoch 3/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 1.2009 - mean_absolute_error: 0.9052 - val_loss: 1.2221 - val_mean_absolute_error: 0.9121\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 1.0553 - mean_absolute_error: 0.8356 - val_loss: 1.1579 - val_mean_absolute_error: 0.8651\n",
      "Epoch 5/150\n",
      "54464/80000 [===================>..........] - ETA: 1s - loss: 0.9837 - mean_absolute_error: 0.795980000/80000 [==============================] - 5s 59us/step - loss: 0.9767 - mean_absolute_error: 0.7921 - val_loss: 1.0975 - val_mean_absolute_error: 0.8356\n",
      "Epoch 6/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.9083 - mean_absolute_error: 0.7563 - val_loss: 1.0330 - val_mean_absolute_error: 0.8128\n",
      "Epoch 7/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.8489 - mean_absolute_error: 0.7272 - val_loss: 0.9991 - val_mean_absolute_error: 0.7941\n",
      "Epoch 8/150\n",
      "55776/80000 [===================>..........] - ETA: 1s - loss: 0.8027 - mean_absolute_error: 0.704180000/80000 [==============================] - 5s 60us/step - loss: 0.8004 - mean_absolute_error: 0.7036 - val_loss: 0.9788 - val_mean_absolute_error: 0.7872\n",
      "Epoch 9/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.7597 - mean_absolute_error: 0.6833 - val_loss: 0.9617 - val_mean_absolute_error: 0.7756\n",
      "Epoch 10/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.7238 - mean_absolute_error: 0.6653 - val_loss: 0.9558 - val_mean_absolute_error: 0.7690\n",
      "Epoch 11/150\n",
      "54720/80000 [===================>..........] - ETA: 1s - loss: 0.6920 - mean_absolute_error: 0.648680000/80000 [==============================] - 5s 60us/step - loss: 0.6922 - mean_absolute_error: 0.6485 - val_loss: 0.9487 - val_mean_absolute_error: 0.7692\n",
      "Epoch 12/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.6644 - mean_absolute_error: 0.6338 - val_loss: 0.9591 - val_mean_absolute_error: 0.7659\n",
      "Epoch 13/150\n",
      "80000/80000 [==============================] - 5s 60us/step - loss: 0.6390 - mean_absolute_error: 0.6197 - val_loss: 0.9518 - val_mean_absolute_error: 0.7708\n",
      "Epoch 14/150\n",
      "55552/80000 [===================>..........] - ETA: 1s - loss: 0.6129 - mean_absolute_error: 0.604880000/80000 [==============================] - 5s 59us/step - loss: 0.6160 - mean_absolute_error: 0.6070 - val_loss: 0.9566 - val_mean_absolute_error: 0.7660\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/150\n",
      "80000/80000 [==============================] - 6s 76us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 2/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 3/150\n",
      "27040/80000 [=========>....................] - ETA: 3s - loss: 13.6969 - mean_absolute_error: 3.530480000/80000 [==============================] - 5s 64us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n",
      "Epoch 4/150\n",
      "80000/80000 [==============================] - 5s 65us/step - loss: 13.7004 - mean_absolute_error: 3.5284 - val_loss: 13.8335 - val_mean_absolute_error: 3.5359\n"
     ]
    }
   ],
   "source": [
    "k_latents = [20, 30]\n",
    "drop_out = [0.3, 0.5]\n",
    "GMF_results_features_add = []\n",
    "max_age = max(Age)\n",
    "for do in drop_out:\n",
    "  for k in k_latents:\n",
    "    GMF_model = get_gmf_model_gen(max_userid,max_movieid,k,do,max_age)\n",
    "    GMF_model.compile(loss='mse',optimizer=Adamax(),metrics=['mae'])\n",
    "    callbacks = [EarlyStopping('val_loss', patience=3)]\n",
    "    GMF_results_features_add_history = GMF_model.fit([Users, Movies, Gender, Age], Ratings, epochs=150, validation_split=.2, verbose=1, callbacks=callbacks, batch_size = 32)\n",
    "    GMF_results_features_add.append((k, do, GMF_results_features_add_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fV1wPz_zRXz"
   },
   "source": [
    "**Ex 3 Section C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85.0
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 693.0,
     "status": "ok",
     "timestamp": 1.532810808987E12,
     "user": {
      "displayName": "Ran Tsur",
      "photoUrl": "//lh3.googleusercontent.com/-pkQ4mq31yyM/AAAAAAAAAAI/AAAAAAAAABg/OJrKb1aE-5Q/s50-c-k-no/photo.jpg",
      "userId": "106754954225433138332"
     },
     "user_tz": -180.0
    },
    "id": "HTKc2-M2xukz",
    "outputId": "b88a20d8-7416-4a0a-eda4-582973403fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop out of  0.3 and  20  latent dimension in 4 epochs => Average MAE: 3.52835 , Min MAE 3.52835\n",
      "drop out of  0.3 and  30  latent dimension in 4 epochs => Average MAE: 3.52835 , Min MAE 3.52835\n",
      "drop out of  0.5 and  20  latent dimension in 14 epochs => Average MAE: 0.767402311807871 , Min MAE 0.6069800953626633\n",
      "drop out of  0.5 and  30  latent dimension in 4 epochs => Average MAE: 3.52835 , Min MAE 3.52835\n"
     ]
    }
   ],
   "source": [
    "for k, drop_out, history in GMF_results_features_add:\n",
    "  avg = np.mean(history.history['mean_absolute_error'])\n",
    "  epochs = len(history.history['mean_absolute_error'])\n",
    "  minimum = min(history.history['mean_absolute_error'])\n",
    "  print(\"drop out of \", drop_out,\"and \",  k, \" latent dimension in\", epochs, \"epochs =>\",  \"Average MAE:\", avg, \", Min MAE\",minimum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZSDsnOlzY-F"
   },
   "source": [
    "**Ex 3 Section D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgypEFyOzbNX"
   },
   "source": [
    "We add to the GMF model the features: age and gender, and then we got min MAE - 0.551 - which improved the model's results.\n",
    "The reason for the relatively small running time of the model is the high MAE in the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Zzu8jm7zyut"
   },
   "source": [
    "**Ex 3 Section E**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDYXE1N20_EC"
   },
   "source": [
    "The Results Are:\n",
    "\n",
    "1. Simple Matrix Factorization - min. MAE: 0.6166\n",
    "2. Generalized Matrix Factorization - min. MAE: 0.5793\n",
    "3. Neural Collaborative Filtering - min. MAE: 0.739\n",
    "\n",
    "So far, the best results are for the Generalized Matrix Factorization model.\n",
    "\n",
    "We decided to add the features of \"age\" and \"gender\" to the GMF model in order the improved the MAE.\n",
    "\n",
    "We can see, according to Section C that the addition of the features improved the results - from 0.5793 to 0.5551.\n",
    "\n",
    "We would recommend GMF with the features adding as the model with the best results in order to predict movies ratings."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Ass3-ran&mor.ipynb",
   "provenance": [
    {
     "file_id": "1_ctWj6DREsiYqZ2ZLIgetwq2UegHe8kn",
     "timestamp": 1.53012070839E12
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
